{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7df834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial  # 导入partial，用于创建带有部分默认参数的函数\n",
    "import numpy as np  # 导入NumPy库，用于数学运算\n",
    "from dataclasses import dataclass  # 导入dataclass，用于创建包含数据的类\n",
    "\n",
    "def vec_num2repr(val, base, prec, max_val):  # 定义函数，将数值向量转换为其在给定基数和精度下的表示形式\n",
    "    base = float(base)  # 将基数转换为浮点数\n",
    "    bs = val.shape[0]  # 获取val向量的长度\n",
    "    sign = 1 * (val >= 0) - 1 * (val < 0)  # 计算val中每个元素的符号\n",
    "    val = np.abs(val)  # 取val中每个元素的绝对值\n",
    "    max_bit_pos = int(np.ceil(np.log(max_val) / np.log(base)).item())  # 计算最大值在给定基数下的位数\n",
    "\n",
    "    before_decimals = []  # 创建一个空列表，用于存储小数点前的数字\n",
    "    for i in range(max_bit_pos):  # 遍历每一个可能的位置\n",
    "        digit = (val / base**(max_bit_pos - i - 1)).astype(int)  # 计算当前位置的数字\n",
    "        before_decimals.append(digit)  # 将计算得到的数字添加到列表中\n",
    "        val -= digit * base**(max_bit_pos - i - 1)  # 更新val，减去已经表示的部分\n",
    "\n",
    "    before_decimals = np.stack(before_decimals, axis=-1)  # 将列表中的数字堆叠成一个NumPy数组\n",
    "\n",
    "    if prec > 0:  # 如果指定了精度\n",
    "        after_decimals = []  # 创建一个空列表，用于存储小数点后的数字\n",
    "        for i in range(prec):  # 遍历每一个小数位\n",
    "            digit = (val / base**(-i - 1)).astype(int)  # 计算当前小数位的数字\n",
    "            after_decimals.append(digit)  # 将计算得到的数字添加到列表中\n",
    "            val -= digit * base**(-i - 1)  # 更新val，减去已经表示的部分\n",
    "\n",
    "        after_decimals = np.stack(after_decimals, axis=-1)  # 将列表中的数字堆叠成一个NumPy数组\n",
    "        digits = np.concatenate([before_decimals, after_decimals], axis=-1)  # 将小数点前后的数字连接起来\n",
    "    else:\n",
    "        digits = before_decimals  # 如果没有指定精度，则所有的数字都在小数点前\n",
    "    return sign, digits  # 返回符号和数字的数组\n",
    "\n",
    "def vec_repr2num(sign, digits, base, prec, half_bin_correction=True):  # 定义函数，将数字的表示形式转换回数值\n",
    "    base = float(base)  # 将基数转换为浮点数\n",
    "    bs, D = digits.shape  # 获取digits数组的形状\n",
    "    digits_flipped = np.flip(digits, axis=-1)  # 将数字数组翻转，因为计算时从最低位开始\n",
    "    powers = -np.arange(-prec, -prec + D)  # 计算每一位数字对应的幂\n",
    "    val = np.sum(digits_flipped/base**powers, axis=-1)  # 计算数值\n",
    "\n",
    "    if half_bin_correction:  # 如果启用了半位修正\n",
    "        val += 0.5/base**prec  # 在数值中加上半位的修正值\n",
    "\n",
    "    return sign * val  # 返回计算得到的数值，包括符号\n",
    "\n",
    "@dataclass  # 使用dataclass装饰器，简化类的定义\n",
    "class SerializerSettings:  # 定义一个类，用于存储序列化和反序列化的设置\n",
    "    base: int = 10  # 数字表示的基数，默认为10\n",
    "    prec: int = 3  # 小数点后的精度，默认为3\n",
    "    signed: bool = True  # 是否允许负数，默认为True\n",
    "    fixed_length: bool = False  # 是否固定长度，默认为False\n",
    "    max_val: float = 1e7  # 可序列化的最大绝对值，默认为1e7\n",
    "    time_sep: str = ' ,'  # 不同时间步之间的分隔符，默认为' ,'\n",
    "    bit_sep: str = ' '  # 数字之间的分隔符，默认为' '\n",
    "    plus_sign: str = ''  # 正号的字符串表示，默认为空\n",
    "    minus_sign: str = ' -'  # 负号的字符串表示，默认为' -'\n",
    "    half_bin_correction: bool = True  # 是否应用半位修正，默认为True\n",
    "    decimal_point: str = ''  # 小数点的字符串表示，默认为空\n",
    "    missing_str: str = ' Nan'  # 缺失值的字符串表示，默认为' Nan'\n",
    "\n",
    "def serialize_arr(arr, settings: SerializerSettings):  # 定义函数，将数值数组序列化为字符串\n",
    "    assert np.all(np.abs(arr[~np.isnan(arr)]) <= settings.max_val), f\"abs(arr) must be <= max_val,\\\n",
    "         but abs(arr)={np.abs(arr)}, max_val={settings.max_val}\"  # 确保所有数值都在可序列化的范围内\n",
    "    \n",
    "    if not settings.signed:  # 如果不允许负数\n",
    "        assert np.all(arr[~np.isnan(arr)] >= 0), f\"unsigned arr must be >= 0\"  # 确保所有数值都非负\n",
    "        plus_sign = minus_sign = ''  # 正负号的表示为空\n",
    "    else:  # 如果允许负数\n",
    "        plus_sign = settings.plus_sign  # 使用设置中的正号表示\n",
    "        minus_sign = settings.minus_sign  # 使用设置中的负号表示\n",
    "    \n",
    "    vnum2repr = partial(vec_num2repr,base=settings.base,prec=settings.prec,max_val=settings.max_val)  # 创建一个带有部分默认参数的函数\n",
    "    sign_arr, digits_arr = vnum2repr(np.where(np.isnan(arr),np.zeros_like(arr),arr))  # 将数值数组转换为符号和数字的表示\n",
    "    ismissing = np.isnan(arr)  # 检查数组中的每个元素是否为缺失值\n",
    "    \n",
    "    def tokenize(arr):  # 定义函数，将数字数组转换为字符串\n",
    "        return ''.join([settings.bit_sep+str(b) for b in arr])  # 使用分隔符连接数字，形成字符串\n",
    "    \n",
    "    bit_strs = []  # 创建一个空列表，用于存储每个数值的字符串表示\n",
    "    for sign, digits, missing in zip(sign_arr, digits_arr, ismissing):  # 遍历每个数值的符号、数字表示和是否缺失的信息\n",
    "        if not settings.fixed_length:  # 如果不固定长度\n",
    "            # remove leading zeros  移除前导零\n",
    "            nonzero_indices = np.where(digits != 0)[0]  # 找到非零数字的索引\n",
    "            if len(nonzero_indices) == 0:  # 如果所有数字都是零\n",
    "                digits = np.array([0])  # 数字表示为单个零\n",
    "            else:  # 如果存在非零数字\n",
    "                digits = digits[nonzero_indices[0]:]  # 保留从第一个非零数字开始的部分\n",
    "            # add a decimal point  添加小数点\n",
    "            prec = settings.prec  # 获取精度\n",
    "            if len(settings.decimal_point):  # 如果定义了小数点的表示\n",
    "                digits = np.concatenate([digits[:-prec], np.array([settings.decimal_point]), digits[-prec:]])  # 在适当位置插入小数点\n",
    "        digits = tokenize(digits)  # 将数字数组转换为字符串\n",
    "        sign_sep = plus_sign if sign == 1 else minus_sign  # 根据符号选择正负号的表示\n",
    "        if missing:  # 如果当前值是缺失值\n",
    "            bit_strs.append(settings.missing_str)  # 添加缺失值的表示\n",
    "        else:  # 如果当前值不是缺失值\n",
    "            bit_strs.append(sign_sep + digits)  # 添加符号和数字的字符串表示\n",
    "    bit_str = settings.time_sep.join(bit_strs)  # 使用时间分隔符连接所有字符串\n",
    "    bit_str += settings.time_sep # 在最后添加一个时间分隔符，以避免最后一个时间步的数字位数不明确的问题\n",
    "    return bit_str  # 返回序列化后的字符串\n",
    "\n",
    "def deserialize_str(bit_str, settings: SerializerSettings, ignore_last=False, steps=None):  # 定义函数，将字符串反序列化为数值数组\n",
    "    orig_bitstring = bit_str  # 保存原始字符串，用于出错时的参考\n",
    "    bit_strs = bit_str.split(settings.time_sep)  # 使用时间分隔符将字符串分割为多个部分\n",
    "    # remove empty strings  移除空字符串\n",
    "    bit_strs = [a for a in bit_strs if len(a) > 0]  # 保留非空的部分\n",
    "    if ignore_last:  # 如果需要忽略最后一个时间步\n",
    "        bit_strs = bit_strs[:-1]  # 移除最后一个部分\n",
    "    if steps is not None:  # 如果指定了步数\n",
    "        bit_strs = bit_strs[:steps]  # 保留指定数量的部分\n",
    "    vrepr2num = partial(vec_repr2num,base=settings.base,prec=settings.prec,half_bin_correction=settings.half_bin_correction)  # 创建一个带有部分默认参数的函数\n",
    "    max_bit_pos = int(np.ceil(np.log(settings.max_val)/np.log(settings.base)).item())  # 计算最大值在给定基数下的位数\n",
    "    sign_arr = []  # 创建一个空列表，用于存储每个数值的符号\n",
    "    digits_arr = []  # 创建一个空列表，用于存储每个数值的数字表示\n",
    "    try:  # 尝试执行反序列化操作\n",
    "        for i, bit_str in enumerate(bit_strs):  # 遍历每一个分割后的字符串\n",
    "            if bit_str.startswith(settings.minus_sign):  # 如果字符串以负号开始\n",
    "                sign = -1  # 符号为负\n",
    "            elif bit_str.startswith(settings.plus_sign):  # 如果字符串以正号开始\n",
    "                sign = 1  # 符号为正\n",
    "            else:  # 如果字符串既不以正号也不以负号开始\n",
    "                assert settings.signed == False, f\"signed bit_str must start with {settings.minus_sign} or {settings.plus_sign}\"  # 断言设置允许的是无符号数\n",
    "            bit_str = bit_str[len(settings.plus_sign):] if sign==1 else bit_str[len(settings.minus_sign):]  # 移除符号部分\n",
    "            if settings.bit_sep=='':  # 如果没有定义数字之间的分隔符\n",
    "                bits = [b for b in bit_str.lstrip()]  # 将字符串分割为单个字符\n",
    "            else:  # 如果定义了数字之间的分隔符\n",
    "                bits = [b[:1] for b in bit_str.lstrip().split(settings.bit_sep)]  # 使用分隔符分割字符串\n",
    "            if settings.fixed_length:  # 如果固定了长度\n",
    "                assert len(bits) == max_bit_pos+settings.prec, f\"fixed length bit_str must have {max_bit_pos+settings.prec} bits, but has {len(bits)}: '{bit_str}'\"  # 断言字符串的长度符合预期\n",
    "            digits = []  # 创建一个空列表，用于存储数字\n",
    "            for b in bits:  # 遍历每一个字符\n",
    "                if b==settings.decimal_point:  # 如果字符是小数点\n",
    "                    continue  # 跳过小数点\n",
    "                # check if is a digit  检查字符是否是数字\n",
    "                if b.isdigit():  # 如果字符是数字\n",
    "                    digits.append(int(b))  # 将字符转换为数字并添加到列表中\n",
    "                else:  # 如果字符不是数字\n",
    "                    break  # 结束循环\n",
    "            sign_arr.append(sign)  # 添加符号\n",
    "            digits_arr.append(digits)  # 添加数字表示\n",
    "    except Exception as e:  # 如果在反序列化过程中遇到异常\n",
    "        print(f\"Error deserializing {settings.time_sep.join(bit_strs[i-2:i+5])}{settings.time_sep}\\n\\t{e}\")  # 打印错误信息\n",
    "        print(f'Got {orig_bitstring}')  # 打印原始字符串\n",
    "        print(f\"Bitstr {bit_str}, separator {settings.bit_sep}\")  # 打印当前处理的字符串和数字之间的分隔符\n",
    "        # At this point, we have already deserialized some of the bit_strs, so we return those below  此时，我们已经反序列化了一部分字符串，因此下面返回这些结果\n",
    "    if digits_arr:  # 如果成功反序列化了一部分字符串\n",
    "        # add leading zeros to get to equal lengths  添加前导零以使所有数字表示的长度相同\n",
    "        max_len = max([len(d) for d in digits_arr])  # 计算所有数字表示中最长的长度\n",
    "        for i in range(len(digits_arr)):  # 遍历每一个数字表示\n",
    "            digits_arr[i] = [0]*(max_len-len(digits_arr[i])) + digits_arr[i]  # 在前面添加零以达到最长长度\n",
    "        return vrepr2num(np.array(sign_arr), np.array(digits_arr))  # 返回反序列化得到的数值数组\n",
    "    else:  # 如果在第一步就出错了\n",
    "        return None  # 返回None以表示反序列化失败\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90417dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # 导入pandas库，用于数据处理和分析\n",
    "import numpy as np  # 导入numpy库，用于数值计算\n",
    "from collections import defaultdict  # 导入defaultdict，一个提供默认值的字典\n",
    "from sklearn.preprocessing import StandardScaler  # 导入StandardScaler，用于数据标准化\n",
    "import datasets  # 导入datasets库，用于加载和处理数据集\n",
    "from datasets import load_dataset  # 从datasets库中导入load_dataset函数，用于加载数据集\n",
    "import os  # 导入os库，用于处理文件和目录\n",
    "import pickle  # 导入pickle库，用于对象序列化和反序列化\n",
    "\n",
    "# 定义一个字典，指定某些数据集的预测长度\n",
    "fix_pred_len = {\n",
    "    'australian_electricity_demand': 336,  # 澳大利亚电力需求数据集\n",
    "    'pedestrian_counts': 24,  # 行人计数数据集\n",
    "    'traffic_hourly': 168,  # 每小时交通数据集    \n",
    "}\n",
    "\n",
    "def get_benchmark_test_sets():\n",
    "    test_set_dir = \"datasets/monash\"  # 定义存放测试集的目录\n",
    "    if not os.path.exists(test_set_dir):  # 如果目录不存在\n",
    "        os.makedirs(test_set_dir)  # 创建目录\n",
    "\n",
    "    if len(os.listdir(test_set_dir)) > 0:  # 如果目录非空\n",
    "        print(f'Loading test sets from {test_set_dir}')  # 打印加载测试集的消息\n",
    "        test_sets = {}  # 初始化测试集字典\n",
    "        for file in os.listdir(test_set_dir):  # 遍历目录中的文件\n",
    "            test_sets[file.split(\".\")[0]] = pickle.load(open(os.path.join(test_set_dir, file), 'rb'))  # 加载并反序列化测试集\n",
    "        return test_sets\n",
    "    else:  # 如果目录为空\n",
    "        print(f'No files found in {test_set_dir}. You are not using our preprocessed datasets!')  # 打印未找到文件的消息\n",
    "    \n",
    "    benchmarks = {\n",
    "        \"monash_tsf\": datasets.get_dataset_config_names(\"monash_tsf\"),  # 从datasets库中获取monash_tsf数据集的配置名\n",
    "    }\n",
    "\n",
    "    test_sets = defaultdict(list)  # 初始化一个默认值为列表的字典，用于存放测试集\n",
    "    for path in benchmarks:  # 遍历benchmarks字典的键\n",
    "        pred_lens = [24, 48, 96, 192] if path == \"ett\" else [None]  # 根据数据集路径设置预测长度\n",
    "        for name in benchmarks[path]:  # 遍历数据集名称\n",
    "            for pred_len in pred_lens:  # 遍历预测长度\n",
    "                if pred_len is None:  # 如果预测长度未指定\n",
    "                    ds = load_dataset(path, name)  # 直接加载数据集\n",
    "                else:  # 如果预测长度指定\n",
    "                    ds = load_dataset(path, name, multivariate=False, prediction_length=pred_len)  # 加载数据集并指定预测长度\n",
    "                \n",
    "                train_example = ds['train'][0]['target']  # 获取训练集第一个样本的目标值\n",
    "                val_example = ds['validation'][0]['target']  # 获取验证集第一个样本的目标值\n",
    "\n",
    "                if len(np.array(train_example).shape) > 1:  # 如果目标值为多变量\n",
    "                    print(f\"Skipping {name} because it is multivariate\")  # 打印跳过多变量数据集的消息\n",
    "                    continue\n",
    "\n",
    "                pred_len = len(val_example) - len(train_example)  # 计算预测长度\n",
    "                if name in fix_pred_len:  # 如果数据集名称在fix_pred_len字典中\n",
    "                    print(f\"Fixing pred len for {name}: {pred_len} -> {fix_pred_len[name]}\")  # 打印修正预测长度的消息\n",
    "                    pred_len = fix_pred_len[name]  # 修正预测长度\n",
    "\n",
    "                tag = name  # 设置标签为数据集名称\n",
    "                print(\"Processing\", tag)  # 打印处理中的消息\n",
    "\n",
    "                pairs = []  # 初始化用于存放历史数据和目标值的列表\n",
    "                for x in ds['test']:  # 遍历测试集\n",
    "                    if np.isnan(x['target']).any():  # 如果目标值包含NaN\n",
    "                        print(f\"Skipping {name} because it has NaNs\")  # 打印跳过包含NaN的数据集的消息\n",
    "                        break\n",
    "                    history = np.array(x['target'][:-pred_len])  # 获取历史数据\n",
    "                    target = np.array(x['target'][-pred_len:])  # 获取目标值\n",
    "                    pairs.append((history, target))  # 将历史数据和目标值添加到列表中\n",
    "                else:  # 如果没有跳过数据集\n",
    "                    scaler = None  # 初始化缩放器为None\n",
    "                    if path == \"ett\":  # 如果数据集路径为ett\n",
    "                        trainset = np.array(ds['train'][0]['target'])  # 获取训练集目标值\n",
    "                        scaler = StandardScaler().fit(trainset[:,None])  # 训练并获取标准化缩放器\n",
    "                    test_sets[tag] = (pairs, scaler)  # 将数据对和缩放器添加到测试集字典中\n",
    "    \n",
    "    for name in test_sets:  # 遍历测试集字典的键\n",
    "        try:\n",
    "            with open(os.path.join(test_set_dir,f\"{name}.pkl\"), 'wb') as f:  # 打开文件进行写入\n",
    "                pickle.dump(test_sets[name], f)  # 序列化并保存测试集\n",
    "            print(f\"Saved {name}\")  # 打印保存成功的消息\n",
    "        except:\n",
    "            print(f\"Failed to save {name}\")  # 打印保存失败的消息\n",
    "\n",
    "    return test_sets  # 返回测试集字典\n",
    "\n",
    "def get_datasets():\n",
    "    benchmarks = get_benchmark_test_sets()  # 获取基准测试集\n",
    "    # 对基准测试集进行随机打乱\n",
    "    for k, v in benchmarks.items():\n",
    "        x, _scaler = v  # 解包测试集和缩放器（缩放器未使用）\n",
    "        train, test = zip(*x)  # 解包训练和测试数据对\n",
    "        np.random.seed(0)  # 设置随机种子\n",
    "        ind = np.arange(len(train))  # 生成索引数组\n",
    "        ind = np.random.permutation(ind)  # 随机打乱索引数组\n",
    "        train = [train[i] for i in ind]  # 根据打乱的索引重新组织训练数据\n",
    "        test = [test[i] for i in ind]  # 根据打乱的索引重新组织测试数据\n",
    "        benchmarks[k] = [list(train), list(test)]  # 更新基准测试集字典\n",
    "\n",
    "    df = pd.read_csv('data/last_val_mae.csv')  # 读取最后一次验证MAE的CSV文件\n",
    "    df.sort_values(by='mae')  # 根据MAE排序\n",
    "\n",
    "    df_paper = pd.read_csv('data/paper_mae_raw.csv')  # 读取原始论文MAE的CSV文件\n",
    "    datasets = df_paper['Dataset']  # 获取数据集列\n",
    "    name_map = {\n",
    "        'Aus. Electricity Demand' :'australian_electricity_demand',  # 名称映射\n",
    "        'Kaggle Weekly': 'kaggle_web_traffic_weekly',\n",
    "        'FRED-MD': 'fred_md',\n",
    "        'Saugeen River Flow': 'saugeenday',        \n",
    "    }\n",
    "    datasets = [name_map.get(d, d) for d in datasets]  # 应用名称映射\n",
    "    # 将数据集名称转换为小写并用下划线替换空格\n",
    "    datasets = [d.lower().replace(' ', '_') for d in datasets]\n",
    "    df_paper['Dataset'] = datasets  # 更新数据集列\n",
    "    df_paper = df_paper.reset_index(drop=True)  # 重置索引\n",
    "    # 为每个数据集添加最后一次验证MAE到df_paper\n",
    "    for dataset in df_paper['Dataset']:\n",
    "        if dataset in df['dataset'].values:\n",
    "            df_paper.loc[df_paper['Dataset'] == dataset, 'Last Value'] = df[df['dataset'] == dataset]['mae'].values[0]\n",
    "    # 将'-'转换为np.nan\n",
    "    df_paper = df_paper.replace('-', np.nan)\n",
    "    # 将所有值转换为浮点数\n",
    "    for method in df_paper.columns[1:]:\n",
    "        df_paper[method] = df_paper[method].astype(float)\n",
    "    df_paper.to_csv('data/paper_mae.csv', index=False)  # 将处理后的数据保存为CSV文件\n",
    "    # 通过除以最后一次验证MAE来标准化每个方法\n",
    "    for method in df_paper.columns[1:-1]:  # 跳过数据集和最后一个值\n",
    "        df_paper[method] = df_paper[method] / df_paper['Last Value']\n",
    "    # 根据方法的最小MAE排序\n",
    "    df_paper['normalized_min'] = df_paper[df_paper.columns[1:-1]].min(axis=1)\n",
    "    df_paper['normalized_median'] = df_paper[df_paper.columns[1:-1]].median(axis=1)\n",
    "    df_paper = df_paper.sort_values(by='normalized_min')\n",
    "    df_paper = df_paper.reset_index(drop=True)\n",
    "    # 将标准化后的数据保存为CSV文件\n",
    "    df_paper.to_csv('data/paper_mae_normalized.csv', index=False)\n",
    "    return benchmarks  # 返回基准测试集字典\n",
    "\n",
    "def main():\n",
    "    get_datasets()  # 调用get_datasets函数\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()  # 如果是主程序，则执行main函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea21fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 导入numpy库，用于数值计算\n",
    "import numbers  # 导入numbers库，用于数字类型检查\n",
    "import random  # 导入random库，用于生成随机数\n",
    "from collections import defaultdict  # 导入defaultdict，用于创建带有默认值的字典\n",
    "from collections.abc import Iterable  # 导入Iterable，用于判断对象是否可迭代\n",
    "\n",
    "import itertools  # 导入itertools库，用于实现各种迭代器\n",
    "import operator  # 导入operator库，提供一系列对操作符的封装\n",
    "import functools  # 导入functools库，用于高阶函数和可调用对象的操作\n",
    "\n",
    "class FixedNumpySeed(object):  # 定义一个固定Numpy随机种子的类\n",
    "    def __init__(self, seed):  # 类的初始化方法\n",
    "        self.seed = seed  # 保存随机种子值\n",
    "    def __enter__(self):  # 进入上下文管理器的方法\n",
    "        self.np_rng_state = np.random.get_state()  # 保存numpy随机状态\n",
    "        np.random.seed(self.seed)  # 设置numpy的随机种子\n",
    "        self.rand_rng_state = random.getstate()  # 保存random模块的随机状态\n",
    "        random.seed(self.seed)  # 设置random模块的随机种子\n",
    "    def __exit__(self, *args):  # 退出上下文管理器的方法\n",
    "        np.random.set_state(self.np_rng_state)  # 恢复numpy的随机状态\n",
    "        random.setstate(self.rand_rng_state)  # 恢复random模块的随机状态\n",
    "\n",
    "class ReadOnlyDict(dict):  # 定义一个只读字典类，继承自dict\n",
    "    def __readonly__(self, *args, **kwargs):  # 定义一个只读的方法\n",
    "        raise RuntimeError(\"Cannot modify ReadOnlyDict\")  # 抛出运行时错误，不允许修改\n",
    "    __setitem__ = __readonly__  # 设置字典项时调用只读方法\n",
    "    __delitem__ = __readonly__  # 删除字典项时调用只读方法\n",
    "    pop = __readonly__  # pop方法调用只读方法\n",
    "    popitem = __readonly__  # popitem方法调用只读方法\n",
    "    clear = __readonly__  # clear方法调用只读方法\n",
    "    update = __readonly__  # update方法调用只读方法\n",
    "    setdefault = __readonly__  # setdefault方法调用只读方法\n",
    "    del __readonly__  # 删除只读方法定义，确保不会被外部调用\n",
    "\n",
    "class NoGetItLambdaDict(dict):  # 定义一个不允许获取lambda函数和非字符串可迭代对象的字典类\n",
    "    def __init__(self,d={}):  # 类的初始化方法\n",
    "        super().__init__()  # 调用父类的初始化方法\n",
    "        for k,v in d.items():  # 遍历传入的字典项\n",
    "            if isinstance(v,dict):  # 如果值是字典类型\n",
    "                self[k] = NoGetItLambdaDict(v)  # 递归创建NoGetItLambdaDict对象\n",
    "            else:  # 如果值不是字典类型\n",
    "                self[k] = v  # 直接设置值\n",
    "    def __getitem__(self, key):  # 重写获取字典项的方法\n",
    "        value = super().__getitem__(key)  # 调用父类的方法获取值\n",
    "        if callable(value) and value.__name__ == \"<lambda>\":  # 如果值是lambda函数\n",
    "            raise LookupError(\"You shouldn't try to retrieve lambda {} from this dict\".format(value))  # 抛出查找错误，不允许获取lambda函数\n",
    "        if isinstance(value,Iterable) and not isinstance(value,(str,bytes,dict,tuple)):  # 如果值是非字符串的可迭代对象\n",
    "            raise LookupError(\"You shouldn't try to retrieve iterable {} from this dict\".format(value))  # 抛出查找错误，不允许获取非字符串的可迭代对象\n",
    "        return value  # 返回值\n",
    "\n",
    "def sample_config(config_spec):  # 定义一个函数，用于从配置规范中生成配置\n",
    "    cfg_all = config_spec  # 将配置规范赋值给cfg_all\n",
    "    more_work=True  # 初始化一个标志，表示是否需要继续处理\n",
    "    i=0  # 初始化计数器\n",
    "    while more_work:  # 当需要继续处理时\n",
    "        cfg_all, more_work = _sample_config(cfg_all,NoGetItLambdaDict(cfg_all))  # 调用_sample_config函数处理配置，并更新more_work标志\n",
    "        i+=1  # 更新计数器\n",
    "        if i>10:  # 如果尝试次数超过10次\n",
    "            raise RecursionError(\"config dependency unresolvable with {}\".format(cfg_all))  # 抛出递归错误，表示配置依赖无法解决\n",
    "    out = defaultdict(dict)  # 创建一个默认值为字典的defaultdict对象\n",
    "    out.update(cfg_all)  # 更新out对象，将处理后的配置赋值给它\n",
    "    return out  # 返回处理后的配置\n",
    "\n",
    "def _sample_config(config_spec,cfg_all):  # 定义一个辅助函数，用于处理配置规范和当前配置\n",
    "    cfg = {}  # 初始化一个空字典，用于存储处理后的配置\n",
    "    more_work = False  # 初始化一个标志，表示是否需要继续处理\n",
    "    for k,v in config_spec.items():  # 遍历配置规范中的项\n",
    "        if isinstance(v,dict):  # 如果值是字典类型\n",
    "            new_dict,extra_work = _sample_config(v,cfg_all)  # 递归调用_sample_config函数处理字典类型的值\n",
    "            cfg[k] = new_dict  # 将处理后的字典赋值给cfg\n",
    "            more_work |= extra_work  # 更新more_work标志\n",
    "        elif isinstance(v,Iterable) and not isinstance(v,(str,bytes,dict,tuple)):  # 如果值是非字符串的可迭代对象\n",
    "            cfg[k] = random.choice(v)  # 从可迭代对象中随机选择一个值赋值给cfg\n",
    "        elif callable(v) and v.__name__ == \"<lambda>\":  # 如果值是lambda函数\n",
    "            try:cfg[k] = v(cfg_all)  # 尝试调用lambda函数，并将结果赋值给cfg\n",
    "            except (KeyError, LookupError,Exception):  # 如果调用lambda函数时发生错误\n",
    "                cfg[k] = v  # 将lambda函数本身赋值给cfg\n",
    "                more_work = True  # 更新more_work标志，表示需要继续处理\n",
    "        else: cfg[k] = v  # 如果值不是字典、非字符串可迭代对象或lambda函数，直接赋值给cfg\n",
    "    return cfg, more_work  # 返回处理后的配置和more_work标志\n",
    "\n",
    "\n",
    "def flatten(d, parent_key='', sep='/'):  # 定义一个函数，用于将嵌套的字典扁平化\n",
    "    items = []  # 初始化一个空列表，用于存储扁平化后的项\n",
    "    for k, v in d.items():  # 遍历字典中的项\n",
    "        new_key = parent_key + sep + k if parent_key else k  # 生成新的键，将嵌套的键连接起来\n",
    "        if isinstance(v, dict) and v: # 如果值是非空字典\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())  # 递归调用flatten函数处理字典类型的值，并将结果扩展到items列表中\n",
    "        else:  # 如果值不是字典类型\n",
    "            items.append((new_key, v))  # 将项添加到items列表中\n",
    "    return dict(items)  # 将items列表转换为字典并返回\n",
    "# flatten简单来说就是将字典里面的嵌套字典转化为非嵌套字典\n",
    "# nested_dict = {\n",
    "#     'a': 1,\n",
    "#     'b': {'b1': 2, 'b2': 3},\n",
    "#     'c': {'c1': {'c11': 4, 'c12': 5}, 'c2': 6}\n",
    "# }\n",
    "# {\n",
    "#     'a': 1, \n",
    "#     'b/b1': 2, \n",
    "#     'b/b2': 3, \n",
    "#     'c/c1/c11': 4, \n",
    "#     'c/c1/c12': 5, \n",
    "#     'c/c2': 6\n",
    "# }\n",
    "\n",
    "def unflatten(d,sep='/'):  # 定义一个函数，用于将扁平化的字典还原为嵌套的字典\n",
    "    out_dict={}  # 初始化一个空字典，用于存储还原后的嵌套字典\n",
    "    for k,v in d.items():  # 遍历扁平化的字典中的项\n",
    "        if isinstance(k,str):  # 如果键是字符串类型\n",
    "            keys = k.split(sep)  # 将键分割为多个部分\n",
    "            dict_to_modify = out_dict  # 初始化一个变量，用于指向需要修改的字典\n",
    "            for partial_key in keys[:-1]:  # 遍历除了最后一个部分之外的所有部分\n",
    "                try: dict_to_modify = dict_to_modify[partial_key]  # 尝试获取对应的子字典\n",
    "                except KeyError:  # 如果子字典不存在\n",
    "                    dict_to_modify[partial_key] = {}  # 创建一个新的子字典\n",
    "                    dict_to_modify = dict_to_modify[partial_key]  # 更新需要修改的字典为新创建的子字典\n",
    "                # Base level reached  达到基础层级\n",
    "            if keys[-1] in dict_to_modify:  # 如果最后一个部分的键已经存在于字典中\n",
    "                dict_to_modify[keys[-1]].update(v)  # 更新对应的值\n",
    "            else:  # 如果最后一个部分的键不存在于字典中\n",
    "                dict_to_modify[keys[-1]] = v  # 创建一个新的项\n",
    "        else: out_dict[k]=v  # 如果键不是字符串类型，直接赋值\n",
    "    return out_dict  # 返回还原后的嵌套字典\n",
    "\n",
    "class grid_iter(object):  # 定义一个类，用于实现对配置规范中网格变量的迭代\n",
    "    def __init__(self,config_spec,num_elements=-1,shuffle=True):  # 类的初始化方法\n",
    "        self.cfg_flat = flatten(config_spec)  # 将配置规范扁平化 将hypers进行扁平化\n",
    "        is_grid_iterable = lambda v: (isinstance(v,Iterable) and not isinstance(v,(str,bytes,dict,tuple)))  # 定义一个函数，用于判断值是否是网格可迭代对象\n",
    "        iterables = sorted({k:v for k,v in self.cfg_flat.items() if is_grid_iterable(v)}.items())  # 从扁平化的配置中筛选出网格可迭代对象\n",
    "        if iterables: self.iter_keys,self.iter_vals = zip(*iterables)  # 如果存在网格可迭代对象，分别获取键和值\n",
    "        else: self.iter_keys,self.iter_vals = [],[[]]  # 如果不存在网格可迭代对象，初始化为空列表\n",
    "        self.vals = list(itertools.product(*self.iter_vals))  # 使用itertools.product生成所有可能的组合\n",
    "        if shuffle:  # 如果需要打乱顺序\n",
    "            with FixedNumpySeed(0): random.shuffle(self.vals)  # 使用固定的随机种子打乱组合的顺序\n",
    "        self.num_elements = num_elements if num_elements>=0 else (-1*num_elements)*len(self)  # 设置迭代的元素数量\n",
    "\n",
    "    def __iter__(self):  # 定义迭代器的__iter__方法\n",
    "        self.i=0  # 初始化计数器\n",
    "        self.vals_iter = iter(self.vals)  # 创建组合的迭代器\n",
    "        return self  # 返回自身作为迭代器\n",
    "    def __next__(self):  # 定义迭代器的__next__方法\n",
    "        self.i+=1  # 更新计数器\n",
    "        if self.i > self.num_elements: raise StopIteration  # 如果达到设定的元素数量，抛出StopIteration异常\n",
    "        if not self.vals: v = []  # 如果没有组合，设置v为空列表\n",
    "        else:  # 如果有组合\n",
    "            try: v = next(self.vals_iter)  # 尝试获取下一个组合\n",
    "            except StopIteration:  # 如果组合已经遍历完毕\n",
    "                self.vals_iter = iter(self.vals)  # 重新创建组合的迭代器\n",
    "                v = next(self.vals_iter)  # 获取下一个组合\n",
    "        chosen_iter_params = dict(zip(self.iter_keys,v))  # 将键和值组合为字典\n",
    "        self.cfg_flat.update(chosen_iter_params)  # 更新扁平化的配置\n",
    "        return sample_config(unflatten(self.cfg_flat))  # 返回处理后的配置\n",
    "    def __len__(self):  # 定义迭代器的__len__方法\n",
    "        product = functools.partial(functools.reduce, operator.mul)  # 使用functools.reduce和operator.mul计算组合的总数\n",
    "        return product(len(v) for v in self.iter_vals) if self.vals else 1  # 返回组合的总数\n",
    "\n",
    "def flatten_dict(d):  # 定义一个函数，用于将字典扁平化，忽略外部键\n",
    "    out = {}  # 初始化一个空字典，用于存储扁平化后的项\n",
    "    for k,v in d.items():  # 遍历字典中的项\n",
    "        if isinstance(v,dict):  # 如果值是字典类型\n",
    "            out.update(flatten_dict(v))  # 递归调用flatten_dict函数处理字典类型的值，并更新out字典\n",
    "        elif isinstance(v,(numbers.Number,str,bytes)):  # 如果值是数字、字符串或字节类型\n",
    "            out[k] = v  # 直接赋值给out字典\n",
    "        else:  # 如果值是其他类型\n",
    "            out[k] = str(v)  # 将值转换为字符串后赋值给out字典\n",
    "    return out  # 返回扁平化后的字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21865d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库和函数\n",
    "import numpy as np  # 用于数值计算\n",
    "from tqdm.auto import tqdm  # 用于显示进度条\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # 用于并行执行\n",
    "from models.utils import grid_iter  # 用于生成超参数的迭代器\n",
    "from dataclasses import is_dataclass  # 检查对象是否为数据类\n",
    "from typing import Any  # 用于类型注解\n",
    "\n",
    "# 定义一个函数，用于将训练集划分为训练和验证集\n",
    "def make_validation_dataset(train, n_val, val_length):\n",
    "    assert isinstance(train, list), 'Train should be a list of series'  # 确保train是列表类型\n",
    "\n",
    "    train_minus_val_list, val_list = [], []  # 初始化存放训练集和验证集的列表\n",
    "    if n_val is None:\n",
    "        n_val = len(train)  # 如果未指定验证样本数量，则使用训练集的数量\n",
    "    for train_series in train[:n_val]:  # 遍历训练集中的前n_val个序列\n",
    "        train_len = max(len(train_series) - val_length, 1)  # 确定每个序列用于训练的长度  val_length表示验证集的长度 要么是test 要么是train训练集的一半\n",
    "        train_minus_val, val = train_series[:train_len], train_series[train_len:]  # 划分训练集和验证集\n",
    "        print(f'Train length: {len(train_minus_val)}, Val length: {len(val)}')  # 打印训练集和验证集的长度\n",
    "        train_minus_val_list.append(train_minus_val)  # 添加到相应的列表中\n",
    "        val_list.append(val)\n",
    "\n",
    "    return train_minus_val_list, val_list, n_val  # 返回划分后的训练集、验证集和验证样本数量\n",
    "\n",
    "# 定义一个函数，用于评估一组超参数在验证集上的性能\n",
    "def evaluate_hyper(hyper, train_minus_val, val, get_predictions_fn):\n",
    "    assert isinstance(train_minus_val, list) and isinstance(val, list), 'Train minus val and val should be lists of series'\n",
    "    return get_predictions_fn(train_minus_val, val, **hyper, num_samples=0)['NLL/D']  # 调用预测函数并返回NLL/D值\n",
    "\n",
    "# 定义一个函数，用于自动调优超参数，并获取最优超参数下的预测结果\n",
    "def get_autotuned_predictions_data(train, test, hypers, num_samples, get_predictions_fn, verbose=False, parallel=True, n_train=None, n_val=None):\n",
    "    if isinstance(hypers,dict):\n",
    "        hypers = list(grid_iter(hypers))  # 如果hypers为字典，则通过grid_iter生成超参数组合的列表\n",
    "    else:\n",
    "        assert isinstance(hypers, list), 'hypers must be a list or dict'  # 确保hypers为列表或字典\n",
    "    if not isinstance(train, list):\n",
    "        train = [train]  # 确保train和test为列表类型\n",
    "        test = [test]\n",
    "    if n_val is None:\n",
    "        n_val = len(train)  # 如果未指定验证样本数量，则使用训练集的长度\n",
    "    if len(hypers) > 1:  # 如果有多组超参数需要评估\n",
    "        val_length = min(len(test[0]), int(np.mean([len(series) for series in train])/2))  # 确定验证集长度 要么是训练集的一半 要么是测试集的长度\n",
    "        train_minus_val, val, n_val = make_validation_dataset(train, n_val=n_val, val_length=val_length)  # 划分训练集和验证集 n_val表示验证集的数量 val_length表示验证集的长度\n",
    "        # 过滤掉长度不足的验证序列\n",
    "        train_minus_val, val = zip(*[(train_series, val_series) for train_series, val_series in zip(train_minus_val, val) if len(val_series) == val_length]) #有些验证集的数量过短 没有val_length那么长 就删掉\n",
    "        train_minus_val = list(train_minus_val)#两个都变为list\n",
    "        val = list(val)\n",
    "        if len(train_minus_val) <= int(0.9*n_val):  # 如果过滤后的验证集数量过少，则抛出异常\n",
    "            raise ValueError(f'Removed too many validation series. Only {len(train_minus_val)} out of {len(n_val)} series have length >= {val_length}. Try or decreasing val_length.')\n",
    "        val_nlls = []  # 初始化存放每组超参数NLL值的列表\n",
    "        def eval_hyper(hyper):  # 定义一个内部函数，用于评估单组超参数\n",
    "            try:\n",
    "                return hyper, evaluate_hyper(hyper, train_minus_val, val, get_predictions_fn)#evaluate_hyper评估超参数怎么样 get_predictions_fn是大模型预测评估函数get_llmtime_predictions_data\n",
    "            except ValueError:\n",
    "                return hyper, float('inf')  # 如果评估过程中出现异常，则返回无穷大\n",
    "            \n",
    "        best_val_nll = float('inf')  # 初始化最佳NLL值\n",
    "        best_hyper = None  # 初始化最佳超参数组合\n",
    "        if not parallel:  # 如果不使用并行计算\n",
    "            for hyper in tqdm(hypers, desc='Hyperparameter search'):  # 遍历超参数组合\n",
    "                _,val_nll = eval_hyper(hyper)  # 评估超参数\n",
    "                val_nlls.append(val_nll)  # 添加NLL值到列表\n",
    "                if val_nll < best_val_nll:  # 如果当前NLL值优于之前的最佳NLL值\n",
    "                    best_val_nll = val_nll  # 更新最佳NLL值\n",
    "                    best_hyper = hyper  # 更新最佳超参数组合\n",
    "                if verbose:  # 如果开启详细输出模式\n",
    "                    print(f'Hyper: {hyper} \\n\\t Val NLL: {val_nll:3f}')  # 打印当前超参数和对应的NLL值\n",
    "        else:  # 如果使用并行计算\n",
    "            with ThreadPoolExecutor() as executor:  # 创建线程池执行器\n",
    "                futures = [executor.submit(eval_hyper,hyper) for hyper in hypers]  # 提交任务\n",
    "                for future in tqdm(as_completed(futures), total=len(hypers), desc='Hyperparameter search'):  # 等待所有任务完成\n",
    "                    hyper,val_nll = future.result()  # 获取结果\n",
    "                    val_nlls.append(val_nll)  # 添加NLL值到列表\n",
    "                    if val_nll < best_val_nll:  # 如果当前NLL值优于之前的最佳NLL值\n",
    "                        best_val_nll = val_nll  # 更新最佳NLL值\n",
    "                        best_hyper = hyper  # 更新最佳超参数组合\n",
    "                    if verbose:  # 如果开启详细输出模式\n",
    "                        print(f'Hyper: {hyper} \\n\\t Val NLL: {val_nll:3f}')  # 打印当前超参数和对应的NLL值\n",
    "    else:  # 如果只有一组超参数\n",
    "        best_hyper = hypers[0]  # 直接使用这组超参数\n",
    "        best_val_nll = float('inf')  # 设置NLL值为无穷大（未评估）\n",
    "    print(f'Sampling with best hyper... {best_hyper} \\n with NLL {best_val_nll:3f}')  # 打印最佳超参数和对应的NLL值\n",
    "    out = get_predictions_fn(train, test, **best_hyper, num_samples=num_samples, n_train=n_train, parallel=parallel)  # 使用最佳超参数获取预测结果\n",
    "    out['best_hyper']=convert_to_dict(best_hyper)  # 将最佳超参数组合转换为字典格式并添加到输出中\n",
    "    return out  # 返回包含预测结果和最佳超参数的字典\n",
    "\n",
    "# 定义一个函数，用于将对象转换为字典格式\n",
    "def convert_to_dict(obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):  # 如果对象是字典\n",
    "        return {k: convert_to_dict(v) for k, v in obj.items()}  # 递归转换每个值\n",
    "    elif isinstance(obj, list):  # 如果对象是列表\n",
    "        return [convert_to_dict(elem) for elem in obj]  # 递归转换每个元素\n",
    "    elif is_dataclass(obj):  # 如果对象是数据类\n",
    "        return convert_to_dict(obj.__dict__)  # 转换数据类的__dict__属性\n",
    "    else:  # 如果对象是其他类型\n",
    "        return obj  # 直接返回对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "from tqdm import tqdm  # 进度条库，用于显示循环的进度\n",
    "from data.serialize import serialize_arr, deserialize_str, SerializerSettings  # 数据序列化和反序列化函数及设置\n",
    "from concurrent.futures import ThreadPoolExecutor  # 线程池，用于并行处理\n",
    "import numpy as np  # 数值计算库\n",
    "import pandas as pd  # 数据处理库\n",
    "from dataclasses import dataclass  # 数据类，用于定义类更简洁\n",
    "from models.llms import completion_fns, nll_fns, tokenization_fns, context_lengths  # 导入模型相关的函数和设置\n",
    "\n",
    "# 定义一个步骤乘数，用于调整预测的步数\n",
    "STEP_MULTIPLIER = 1.2\n",
    "\n",
    "# 定义一个数据缩放器类\n",
    "@dataclass\n",
    "class Scaler:\n",
    "    \"\"\"\n",
    "    表示一个数据缩放器，具有转换和逆转换函数的属性。\n",
    "\n",
    "    属性:\n",
    "        transform (callable): 应用转换的函数。\n",
    "        inv_transform (callable): 应用逆转换的函数。\n",
    "    \"\"\"\n",
    "    transform: callable = lambda x: x  # 默认的转换函数，不做任何处理\n",
    "    inv_transform: callable = lambda x: x  # 默认的逆转换函数，不做任何处理\n",
    "\n",
    "# 定义一个函数，基于给定的历史数据生成一个Scaler对象\n",
    "def get_scaler(history, alpha=0.95, beta=0.3, basic=False):\n",
    "    \"\"\"\n",
    "    根据给定的历史数据生成一个Scaler对象。\n",
    "\n",
    "    参数:\n",
    "        history (array-like): 用于派生缩放的数据。\n",
    "        alpha (float, optional): 缩放的分位数，默认为0.95。\n",
    "        beta (float, optional): 偏移参数，默认为0.3。\n",
    "        basic (bool, optional): 如果为True，不应用偏移，避免按小于0.01的值缩放，默认为False。\n",
    "\n",
    "    返回:\n",
    "        Scaler: 配置好的缩放器对象。\n",
    "    \"\"\"\n",
    "    history = history[~np.isnan(history)]  # 移除NaN值\n",
    "    if basic:\n",
    "        # 基础缩放：仅基于分位数缩放，不应用偏移\n",
    "        q = np.maximum(np.quantile(np.abs(history), alpha), .01)\n",
    "        def transform(x):\n",
    "            return x / q\n",
    "        def inv_transform(x):\n",
    "            return x * q\n",
    "    else:\n",
    "        # 高级缩放：基于分位数和偏移量进行缩放\n",
    "        min_ = np.min(history) - beta*(np.max(history)-np.min(history))\n",
    "        q = np.quantile(history-min_, alpha)\n",
    "        if q == 0:\n",
    "            q = 1\n",
    "        def transform(x):\n",
    "            return (x - min_) / q\n",
    "        def inv_transform(x):\n",
    "            return x * q + min_\n",
    "    return Scaler(transform=transform, inv_transform=inv_transform)\n",
    "\n",
    "# 定义一个函数，用于根据给定模型的最大上下文长度截断输入\n",
    "def truncate_input(input_arr, input_str, settings, model, steps):\n",
    "    \"\"\"\n",
    "    根据给定模型的最大上下文长度截断输入。\n",
    "    \n",
    "    参数:\n",
    "        input_arr (array-like): 输入时间序列。\n",
    "        input_str (str): 序列化的输入时间序列。\n",
    "        settings (SerializerSettings): 序列化设置。\n",
    "        model (str): 要使用的LLM模型名称。\n",
    "        steps (int): 要预测的步数。\n",
    "    返回:\n",
    "        tuple: 包含:\n",
    "            - input_arr (array-like): 截断的输入时间序列。\n",
    "            - input_str (str): 截断的序列化输入时间序列。\n",
    "    \"\"\"\n",
    "    # 检查模型是否在支持的模型列表中\n",
    "    if model in tokenization_fns and model in context_lengths:\n",
    "        tokenization_fn = tokenization_fns[model]  # 获取模型的tokenization函数\n",
    "        context_length = context_lengths[model]  # 获取模型支持的最大上下文长度\n",
    "        input_str_chunks = input_str.split(settings.time_sep)  # 根据时间分隔符分割输入字符串\n",
    "        for i in range(len(input_str_chunks) - 1):\n",
    "            truncated_input_str = settings.time_sep.join(input_str_chunks[i:])  # 从当前位置开始截断输入字符串\n",
    "            if not truncated_input_str.endswith(settings.time_sep):\n",
    "                truncated_input_str += settings.time_sep  # 确保截断后的字符串以时间分隔符结尾\n",
    "            input_tokens = tokenization_fn(truncated_input_str)  # 将截断后的字符串转换为tokens\n",
    "            num_input_tokens = len(input_tokens)  # 计算输入tokens的数量\n",
    "            avg_token_length = num_input_tokens / (len(input_str_chunks) - i)  # 计算平均token长度\n",
    "            num_output_tokens = avg_token_length * steps * STEP_MULTIPLIER  # 计算输出tokens的预期数量\n",
    "            if num_input_tokens + num_output_tokens <= context_length:\n",
    "                truncated_input_arr = input_arr[i:]  # 截断输入数组\n",
    "                break\n",
    "        if i > 0:\n",
    "            print(f'Warning: Truncated input from {len(input_arr)} to {len(truncated_input_arr)}')\n",
    "        return truncated_input_arr, truncated_input_str\n",
    "    else:\n",
    "        return input_arr, input_str\n",
    "\n",
    "# 定义一个函数处理LLM预测后的输出\n",
    "def handle_prediction(pred, expected_length, strict=False):\n",
    "    \"\"\"\n",
    "    处理LLM预测后的输出，可能过长或过短，或者如果在第一步预测就失败了，则为None。\n",
    "\n",
    "    参数:\n",
    "        pred (array-like or None): 预测值，None表示反序列化失败。\n",
    "        expected_length (int): 预测的期望长度。\n",
    "        strict (bool, optional): 如果为True，对于无效预测返回None。默认为False。\n",
    "\n",
    "    返回:\n",
    "        array-like: 处理后的预测。\n",
    "    \"\"\"\n",
    "    if pred is None:\n",
    "        return None\n",
    "    else:\n",
    "        if len(pred) < expected_length:\n",
    "            if strict:\n",
    "                print(f'Warning: Prediction too short {len(pred)} < {expected_length}, returning None')\n",
    "                return None\n",
    "            else:\n",
    "                print(f'Warning: Prediction too short {len(pred)} < {expected_length}, padded with last value')\n",
    "                return np.concatenate([pred, np.full(expected_length - len(pred), pred[-1])])\n",
    "        else:\n",
    "            return pred[:expected_length]\n",
    "\n",
    "# 定义一个函数，生成并处理来自语言模型的文本完成对输入时间序列的预测\n",
    "def generate_predictions(\n",
    "    completion_fn, \n",
    "    input_strs, \n",
    "    steps, \n",
    "    settings: SerializerSettings, \n",
    "    scalers: None,\n",
    "    num_samples=1, \n",
    "    temp=0.7, \n",
    "    parallel=True,\n",
    "    strict_handling=False,\n",
    "    max_concurrent=10,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    从语言模型获取文本完成，生成输入时间序列的预测并处理这些预测。\n",
    "\n",
    "    参数:\n",
    "        completion_fn (callable): 从LLM获取文本完成的函数。\n",
    "        input_strs (list of array-like): 输入时间序列列表。\n",
    "        steps (int): 预测的步数。\n",
    "        settings (SerializerSettings): 序列化设置。\n",
    "        scalers (list of Scaler, optional): Scaler对象列表。默认为None，意味着不应用缩放。\n",
    "        num_samples (int, optional): 返回的样本数量。默认为1。\n",
    "        temp (float, optional): 抽样的温度。默认为0.7。\n",
    "        parallel (bool, optional): 如果为True，以并行方式运行完成。默认为True。\n",
    "        strict_handling (bool, optional): 如果为True，对格式不正确或预期长度不正确的预测返回None。默认为False。\n",
    "        max_concurrent (int, optional): 并发完成的最大数量。默认为10。\n",
    "        **kwargs: 其他关键字参数。\n",
    "\n",
    "    返回:\n",
    "        tuple: 包含：\n",
    "            - preds (list of lists): 数值预测。\n",
    "            - completions_list (list of lists): 原始文本完成。\n",
    "            - input_strs (list of str): 序列化输入字符串。\n",
    "    \"\"\"\n",
    "    completions_list = []\n",
    "    complete = lambda x: completion_fn(input_str=x, steps=steps*STEP_MULTIPLIER, settings=settings, num_samples=num_samples,\n",
    "temp=temp, **kwargs)  # 定义一个lambda函数用于获取文本完成\n",
    "    if parallel and len(input_strs) > 1:  # 如果启用并行处理且输入字符串列表长度大于1\n",
    "        print('Running completions in parallel for each input')\n",
    "        with ThreadPoolExecutor(min(max_concurrent, len(input_strs))) as p:  # 创建一个线程池\n",
    "            completions_list = list(tqdm(p.map(complete, input_strs), total=len(input_strs)))  # 并行执行complete函数并显示进度\n",
    "    else:  # 如果不并行或输入字符串列表长度不大于1\n",
    "        completions_list = [complete(input_str) for input_str in tqdm(input_strs)]  # 顺序执行complete函数并显示进度\n",
    "    def completion_to_pred(completion, inv_transform):\n",
    "        # 定义一个函数将文本完成转换为预测值\n",
    "        pred = handle_prediction(deserialize_str(completion, settings, ignore_last=False, steps=steps), expected_length=steps, strict=strict_handling)\n",
    "        if pred is not None:  # 如果预测值非None\n",
    "            return inv_transform(pred)  # 应用逆变换函数\n",
    "        else:  # 如果预测值为None\n",
    "            return None\n",
    "    preds = [[completion_to_pred(completion, scaler.inv_transform) for completion in completions] for completions, scaler in zip(completions_list, scalers)]\n",
    "    # 将文本完成列表和缩放器列表转换为预测值列表\n",
    "    return preds, completions_list, input_strs\n",
    "\n",
    "def get_llmtime_predictions_data(train, test, model, settings, num_samples=10, temp=0.7, alpha=0.95, beta=0.3, basic=False, parallel=True, **kwargs):\n",
    "    \"\"\"\n",
    "    基于训练序列(历史)获得LLM的预测，并在测试序列(真实未来)上评估似然。\n",
    "    train和test可以是单个时间序列或时间序列的列表。\n",
    "\n",
    "    参数:\n",
    "        train (array-like or list of array-like): 训练时间序列数据(历史)。\n",
    "        test (array-like or list of array-like): 测试时间序列数据(真实未来)。\n",
    "        model (str): 要使用的LLM模型名称。必须在completion_fns中有对应项。\n",
    "        settings (SerializerSettings or dict): 序列化设置。\n",
    "        num_samples (int, optional): 返回的样本数量。默认为10。\n",
    "        temp (float, optional): 抽样的温度。默认为0.7。\n",
    "        alpha (float, optional): 缩放参数。默认为0.95。\n",
    "        beta (float, optional): 偏移参数。默认为0.3。\n",
    "        basic (bool, optional): 如果为True，使用基础版本的数据缩放。默认为False。\n",
    "        parallel (bool, optional): 如果为True，以并行方式运行预测。默认为True。\n",
    "        **kwargs: 其他关键字参数。\n",
    "\n",
    "    返回:\n",
    "        dict: 包含预测、样本、中位数、NLL/D平均值以及其他相关信息的字典。\n",
    "    \"\"\"\n",
    "\n",
    "    assert model in completion_fns, f'Invalid model {model}, must be one of {list(completion_fns.keys())}'  # 检查模型是否有效\n",
    "    completion_fn = completion_fns[model]  # 获取模型对应的完成函数\n",
    "    nll_fn = nll_fns[model] if model in nll_fns else None  # 获取模型对应的NLL函数\n",
    "    \n",
    "    if isinstance(settings, dict):  # 如果settings是字典\n",
    "        settings = SerializerSettings(**settings)  # 将settings转换为SerializerSettings对象\n",
    "    if not isinstance(train, list):  # 如果train不是列表\n",
    "        train = [train]  # 将train转换为列表\n",
    "        test = [test]  # 将test转换为列表\n",
    "\n",
    "    for i in range(len(train)):  # 遍历train列表\n",
    "        if not isinstance(train[i], pd.Series):  # 如果train的元素不是pd.Series\n",
    "            train[i] = pd.Series(train[i], index=pd.RangeIndex(len(train[i])))  # 将train的元素转换为pd.Series\n",
    "            test[i] = pd.Series(test[i], index=pd.RangeIndex(len(train[i]), len(test[i])+len(train[i])))  # 将test的元素转换为pd.Series\n",
    "\n",
    "    test_len = len(test[0])  # 获取测试序列的长度\n",
    "    assert all(len(t)==test_len for t in test), f'All test series must have same length, got {[len(t) for t in test]}'\n",
    "\n",
    "    # 为每个序列创建一个独特的缩放器\n",
    "    scalers = [get_scaler(train[i].values, alpha=alpha, beta=beta, basic=basic) for i in range(len(train))]\n",
    "\n",
    "    # 转换输入序列\n",
    "    input_arrs = [train[i].values for i in range(len(train))]\n",
    "    transformed_input_arrs = [scaler.transform(input_array) for input_array, scaler in zip(input_arrs, scalers)]\n",
    "    # 序列化输入序列\n",
    "    input_strs = [serialize_arr(scaled_input_arr, settings) for scaled_input_arr in transformed_input_arrs]\n",
    "    # 根据模型的最大上下文长度截断输入序列\n",
    "    input_arrs, input_strs = zip(*[truncate_input(input_array, input_str, settings, model, test_len) for input_array, input_str in zip(input_arrs, input_strs)])\n",
    "\n",
    "    steps = test_len  # 设置预测步数\n",
    "    samples = None\n",
    "    medians = None\n",
    "    completions_list = None\n",
    "    if num_samples > 0:  # 如果需要返回样本\n",
    "        preds, completions_list, input_strs = generate_predictions(completion_fn, input_strs, steps, settings, scalers,\n",
    "                                                                    num_samples=num_samples, temp=temp, \n",
    "                                                                    parallel=parallel, **kwargs)\n",
    "        # 将预测结果转换为pandas DataFrame\n",
    "        samples = [pd.DataFrame(preds[i], columns=test[i].index) for i in range(len(preds))]\n",
    "        # 计算中位数\n",
    "        medians = [sample.median(axis=0) for sample in samples]\n",
    "        samples = samples if len(samples) > 1 else samples[0]  # 如果样本数大于1，则返回样本列表，否则返回单个样本\n",
    "        medians = medians if len(medians) > 1 else medians[0]  # 如果中位数数大于1，则返回中位数列表，否则返回单个中位数\n",
    "\n",
    "    out_dict = {\n",
    "        'samples': samples,\n",
    "        'median': medians,\n",
    "        'info': {\n",
    "            'Method': model,\n",
    "        },\n",
    "        'completions_list': completions_list,\n",
    "        'input_strs': input_strs,\n",
    "    }\n",
    "    # 计算在真实测试序列上，条件于（截断的）输入序列的NLL/D\n",
    "    if nll_fn is not None:\n",
    "        BPDs = [nll_fn(input_arr=input_arrs[i], target_arr=test[i].values, settings=settings, transform=scalers[i].transform, count_seps=True, temp=temp) for i in range(len(train))]\n",
    "        out_dict['NLL/D'] = np.mean(BPDs)  # 计算NLL/D的平均值\n",
    "\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718daec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # 从functools模块导入partial函数，用于固定函数的一些参数值\n",
    "from models.gpt import gpt_completion_fn, gpt_nll_fn # 从models.gpt模块导入gpt的文本完成和负对数似然函数\n",
    "from models.gpt import tokenize_fn as gpt_tokenize_fn # 从models.gpt模块导入tokenize函数，并重命名为gpt_tokenize_fn\n",
    "from models.llama import llama_completion_fn, llama_nll_fn # 从models.llama模块导入llama的文本完成和负对数似然函数\n",
    "from models.llama import tokenize_fn as llama_tokenize_fn # 从models.llama模块导入tokenize函数，并重命名为llama_tokenize_fn\n",
    "\n",
    "from models.mistral import mistral_completion_fn, mistral_nll_fn # 从models.mistral模块导入mistral的文本完成和负对数似然函数\n",
    "from models.mistral import tokenize_fn as mistral_tokenize_fn # 从models.mistral模块导入tokenize函数，并重命名为mistral_tokenize_fn\n",
    "\n",
    "from models.mistral_api import mistral_api_completion_fn, mistral_api_nll_fn # 从models.mistral_api模块导入mistral_api的文本完成和负对数似然函数\n",
    "from models.mistral_api import tokenize_fn as mistral_api_tokenize_fn # 从models.mistral_api模块导入tokenize函数，并重命名为mistral_api_tokenize_fn\n",
    "\n",
    "\n",
    "# Required: Text completion function for each model\n",
    "# 必需：为每个模型映射一个用于采样文本完成的函数\n",
    "# 每个模型映射到一个采样文本完成的函数。\n",
    "# 完成函数应遵循以下签名：\n",
    "# \n",
    "# 参数：\n",
    "#   - input_str (str): 输入时间序列的字符串表示。\n",
    "#   - steps (int): 预测的步数。\n",
    "#   - settings (SerializerSettings): 序列化设置。\n",
    "#   - num_samples (int): 要采样的完成数量。\n",
    "#   - temp (float): 模型输出随机性的温度参数。\n",
    "# \n",
    "# 返回：\n",
    "#   - list: 从模型中采样的完成字符串列表。\n",
    "completion_fns = {\n",
    "    'text-davinci-003': partial(gpt_completion_fn, model='text-davinci-003'),\n",
    "    'gpt-4': partial(gpt_completion_fn, model='gpt-4'),\n",
    "    'gpt-4-1106-preview':partial(gpt_completion_fn, model='gpt-4-1106-preview'),\n",
    "    'gpt-3.5-turbo-instruct': partial(gpt_completion_fn, model='gpt-3.5-turbo-instruct'),\n",
    "    'mistral': partial(mistral_completion_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_completion_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_completion_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_completion_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_completion_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_completion_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_completion_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_completion_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_completion_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_completion_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: NLL/D functions for each model\n",
    "# 可选：为每个模型映射一个计算连续负对数似然/维度（NLL/D）的函数。这用于仅计算似然，采样时不需要。\n",
    "# \n",
    "# 每个模型映射到一个计算连续负对数似然/维度（NLL/D）的函数。这用于计算似然值，采样时不需要。\n",
    "# \n",
    "# NLL函数应遵循以下签名：\n",
    "# \n",
    "# 参数：\n",
    "#   - input_arr (np.ndarray): 数据转换后的输入时间序列（历史）。\n",
    "#   - target_arr (np.ndarray): 数据转换后的真实时间序列（未来）。\n",
    "#   - settings (SerializerSettings): 序列化设置。\n",
    "#   - transform (callable): 数据转换函数（例如，缩放）用于确定雅可比因子。\n",
    "#   - count_seps (bool): 如果为True，则在NLL计算中计算时间步分隔符，如果允许数字的变化长度则需要。\n",
    "#   - temp (float): 采样的温度参数。\n",
    "# \n",
    "# 返回：\n",
    "#   - float: 给定输入数组条件下目标数组的NLL/D计算值。\n",
    "nll_fns = {\n",
    "    'text-davinci-003': partial(gpt_nll_fn, model='text-davinci-003'),\n",
    "    'mistral': partial(mistral_nll_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_nll_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_nll_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_nll_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_nll_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_nll_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_nll_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_nll_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_nll_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_nll_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: Tokenization function for each model, only needed if you want automatic input truncation.\n",
    "# 可选：为每个模型映射一个tokenization函数，仅在您想要自动输入截断时需要。\n",
    "# Tokenization函数应遵循以下签名：\n",
    "#\n",
    "# 参数：\n",
    "#   - str (str): 要tokenize的字符串。\n",
    "# 返回：\n",
    "#   - token_ids (list): token ids列表。\n",
    "tokenization_fns = {\n",
    "    'text-davinci-003': partial(gpt_tokenize_fn, model='text-davinci-003'),\n",
    "    'gpt-3.5-turbo-instruct': partial(gpt_tokenize_fn, model='gpt-3.5-turbo-instruct'),\n",
    "    'mistral': partial(mistral_tokenize_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_tokenize_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_tokenize_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_tokenize_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_tokenize_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_tokenize_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_tokenize_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_tokenize_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_tokenize_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_tokenize_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: Context lengths for each model, only needed if you want automatic input truncation.\n",
    "# 可选：为每个模型映射一个上下文长度，仅在您想要自动输入截断时需要。\n",
    "context_lengths = {\n",
    "    'text-davinci-003': 4097,\n",
    "    'gpt-3.5-turbo-instruct': 4097,\n",
    "    'mistral-api-tiny': 4097,\n",
    "    'mistral-api-small': 4097,\n",
    "    'mistral-api-medium': 4097,\n",
    "    'mistral': 4096,\n",
    "    'llama-7b': 4096,\n",
    "    'llama-13b': 4096,\n",
    "    'llama-70b': 4096,\n",
    "    'llama-7b-chat': 4096,\n",
    "    'llama-13b-chat': 4096,\n",
    "    'llama-70b-chat': 4096,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.serialize import serialize_arr, SerializerSettings # 从data.serialize模块导入序列化数组函数和序列化设置类\n",
    "import openai # 导入openai库用于访问OpenAI的API\n",
    "import tiktoken # 导入tiktoken库用于进行文本编码和解码\n",
    "import numpy as np # 导入numpy库用于进行数学计算\n",
    "from jax import grad, vmap # 从jax库导入grad和vmap函数，用于计算梯度和对函数进行向量化\n",
    "\n",
    "\n",
    "def tokenize_fn(str, model):\n",
    "    \"\"\"\n",
    "    为特定GPT模型的字符串获取token ID。\n",
    "\n",
    "    参数：\n",
    "        str (str): 要被tokenize的字符串。\n",
    "        model (str): LLM模型的名称。\n",
    "\n",
    "    返回：\n",
    "        list of int: 对应的token ID列表。\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model) # 获取模型对应的编码器\n",
    "    return encoding.encode(str) # 对字符串进行编码并返回token ID\n",
    "\n",
    "def get_allowed_ids(strs, model):\n",
    "    \"\"\"\n",
    "    为特定GPT模型的一系列字符串获取token ID。\n",
    "\n",
    "    参数：\n",
    "        strs (list of str): 要被转换的字符串列表。\n",
    "        model (str): LLM模型的名称。\n",
    "\n",
    "    返回：\n",
    "        list of int: 对应的token ID列表。\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model) # 获取模型对应的编码器\n",
    "    ids = []\n",
    "    for s in strs:\n",
    "        id = encoding.encode(s) # 对每个字符串进行编码\n",
    "        ids.extend(id) # 将编码后的ID添加到列表中\n",
    "    return ids\n",
    "\n",
    "def gpt_completion_fn(model, input_str, steps, settings, num_samples, temp):\n",
    "    \"\"\"\n",
    "    使用OpenAI的API从GPT生成文本完成。\n",
    "\n",
    "    参数：\n",
    "        model (str): 要使用的GPT-3模型的名称。\n",
    "        input_str (str): 序列化的输入时间序列数据。\n",
    "        steps (int): 要预测的时间步数。\n",
    "        settings (SerializerSettings): 序列化设置。\n",
    "        num_samples (int): 要生成的完成数量。\n",
    "        temp (float): 采样的温度。\n",
    "\n",
    "    返回：\n",
    "        list of str: 生成的样本列表。\n",
    "    \"\"\"\n",
    "    avg_tokens_per_step = len(tokenize_fn(input_str, model)) / len(input_str.split(settings.time_sep)) # 计算每个步骤的平均token数量\n",
    "    # 定义logit偏差以防止GPT-3生成不需要的token\n",
    "    logit_bias = {}\n",
    "    allowed_tokens = [settings.bit_sep + str(i) for i in range(settings.base)] \n",
    "    allowed_tokens += [settings.time_sep, settings.plus_sign, settings.minus_sign]\n",
    "    allowed_tokens = [t for t in allowed_tokens if len(t) > 0] # 移除空token，如隐式加号\n",
    "    if (model not in ['gpt-3.5-turbo','gpt-4','gpt-4-1106-preview']): # 对于非聊天模型，支持logit偏差\n",
    "        logit_bias = {id: 30 for id in get_allowed_ids(allowed_tokens, model)}\n",
    "    if model in ['gpt-3.5-turbo','gpt-4','gpt-4-1106-preview']: # 对于聊天模型\n",
    "        chatgpt_sys_message = \"You are a helpful assistant that performs time series predictions. The user will provide a sequence and you will predict the remaining sequence. The sequence is represented by decimal strings separated by commas.\"\n",
    "        extra_input = \"Please continue the following sequence without producing any additional text. Do not say anything like 'the next terms in the sequence are', just return the numbers. Sequence:\\n\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": chatgpt_sys_message},\n",
    "                    {\"role\": \"user\", \"content\": extra_input+input_str+settings.time_sep}\n",
    "                ],\n",
    "            max_tokens=int(avg_tokens_per_step*steps), \n",
    "            temperature=temp,\n",
    "            logit_bias=logit_bias,\n",
    "            n=num_samples,\n",
    "        )\n",
    "        return [choice.message.content for choice in response.choices]\n",
    "    else: # 对于非聊天模型\n",
    "        response = openai.Completion.create(\n",
    "            model=model,\n",
    "            prompt=input_str, \n",
    "            max_tokens=int(avg_tokens_per_step*steps), \n",
    "            temperature=temp,\n",
    "            logit_bias=logit_bias\n",
    "            n=num_samples\n",
    "        )\n",
    "        return [choice.text for choice in response.choices]\n",
    "    \n",
    "def gpt_nll_fn(model, input_arr, target_arr, settings:SerializerSettings, transform, count_seps=True, temp=1):\n",
    "    \"\"\"\n",
    "    根据LLM计算目标数组的每维度负对数似然(NLL)。\n",
    "\n",
    "    参数：\n",
    "        model (str): 要使用的LLM模型名称。\n",
    "        input_arr (array-like): 输入数组（历史数据）。\n",
    "        target_arr (array-like): 目标数组（未来数据）。\n",
    "        settings (SerializerSettings): 序列化设置。\n",
    "        transform (callable): 应用于数值的转换，在序列化前。\n",
    "        count_seps (bool, 可选): 是否在计算中考虑分隔符。对于生成变量位数的模型应为真。默认为True。\n",
    "        temp (float, 可选): 采样的温度。默认为1。\n",
    "\n",
    "    返回：\n",
    "        float: 计算出的每维度NLL。\n",
    "    \"\"\"\n",
    "    input_str = serialize_arr(vmap(transform)(input_arr), settings) # 序列化输入数组\n",
    "    target_str = serialize_arr(vmap(transform)(target_arr), settings) # 序列化目标数组\n",
    "    assert input_str.endswith(settings.time_sep), f'输入字符串必须以{settings.time_sep}结束, 得到的是{input_str}'\n",
    "    full_series = input_str + target_str # 拼接输入和目标序列\n",
    "    response = openai.Completion.create(model=model, prompt=full_series, logprobs=5, max_tokens=0, echo=True, temperature=temp)\n",
    "    logprobs = np.array(response['choices'][0].logprobs.token_logprobs, dtype=np.float32) # 获取log概率\n",
    "    tokens = np.array(response['choices'][0].logprobs.tokens) # 获取生成的tokens\n",
    "    top5logprobs = response['choices'][0].logprobs.top_logprobs # 获取top 5 log概率\n",
    "    seps = tokens == settings.time_sep # 找到时间分隔符的位置\n",
    "    target_start = np.argmax(np.cumsum(seps) == len(input_arr)) + 1 # 定位目标序列开始的位置\n",
    "    logprobs = logprobs[target_start:] # 获取目标序列的log概率\n",
    "    tokens = tokens[target_start:] # 获取目标序列的tokens\n",
    "    top5logprobs = top5logprobs[target_start:] # 获取目标序列的top 5 log概率\n",
    "    seps = tokens == settings.time_sep # 重新找到目标序列中时间分隔符的位置\n",
    "    assert len(logprobs[seps]) == len(target_arr), f'每个目标应有一个分隔符。得到了{len(logprobs[seps])}个分隔符和{len(target_arr)}个目标。'\n",
    "    # 通过移除多余的并重新规范化来调整log概率（见论文附录）\n",
    "    allowed_tokens = [settings.bit_sep + str(i) for i in range(settings.base)] \n",
    "    allowed_tokens += [settings.time_sep, settings.plus_sign, settings.minus_sign, settings.bit_sep+settings.decimal_point]\n",
    "    allowed_tokens = {t for t in allowed_tokens if len(t) > 0}\n",
    "    p_extra = np.array([sum(np.exp(ll) for k,ll in top5logprobs[i].items() if not (k in allowed_tokens)) for i in range(len(top5logprobs))])\n",
    "    if settings.bit_sep == '':\n",
    "        p_extra = 0\n",
    "    adjusted_logprobs = logprobs - np.log(1-p_extra) # 调整后的log概率\n",
    "    digits_bits = -adjusted_logprobs[~seps].sum() # 数字的bits\n",
    "    seps_bits = -adjusted_logprobs[seps].sum() # 分隔符的bits\n",
    "    BPD = digits_bits / len(target_arr) # 每个目标的bits\n",
    "    if count_seps:\n",
    "        BPD += seps_bits / len(target_arr) # 如果计算分隔符，则加上分隔符的bits\n",
    "    # log p(x) = log p(token) - log bin_width = log p(token) + prec * log base\n",
    "    transformed_nll = BPD - settings.prec * np.log(settings.base) # 转换后的NLL\n",
    "    avg_logdet_dydx = np.log(vmap(grad(transform))(target_arr)).mean() # 计算目标数组变换的平均log雅可比行列式\n",
    "    return transformed_nll - avg_logdet_dydx # 返回调整后的NLL减去平均log雅可比行列式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial  # 从functools模块导入partial函数，用于创建偏函数\n",
    "import numpy as np  # 导入numpy库，通常简称为np\n",
    "from dataclasses import dataclass  # 从dataclasses模块导入dataclass装饰器，用于创建数据类\n",
    "\n",
    "def vec_num2repr(val, base, prec, max_val):  # 定义一个函数，将数值转换为指定基数和精度的表示形式\n",
    "    \"\"\"\n",
    "    将数字转换为指定基数和精度的表示形式。\n",
    "\n",
    "    参数:\n",
    "    - val (np.array): 要表示的数字。\n",
    "    - base (int): 表示的基数。\n",
    "    - prec (int): 基数表示中小数点后的精度。\n",
    "    - max_val (float): 数字的最大绝对值。\n",
    "\n",
    "    返回:\n",
    "    - tuple: 指定基数表示形式中的符号和数字。\n",
    "    \n",
    "    示例:\n",
    "        以基数10, 精度2为例:\n",
    "            0.5   ->    50\n",
    "            3.52  ->   352\n",
    "            12.5  ->  1250\n",
    "    \"\"\"\n",
    "    base = float(base)  # 将基数转换为浮点数\n",
    "    bs = val.shape[0]  # 获取val数组的大小\n",
    "    sign = 1 * (val >= 0) - 1 * (val < 0)  # 计算val中每个元素的符号\n",
    "    val = np.abs(val)  # 取val的绝对值\n",
    "    max_bit_pos = int(np.ceil(np.log(max_val) / np.log(base)).item())  # 计算最大值的位数\n",
    "\n",
    "    before_decimals = []  # 初始化小数点前的数字列表\n",
    "    for i in range(max_bit_pos):  # 对于每个位数\n",
    "        digit = (val / base**(max_bit_pos - i - 1)).astype(int)  # 计算每位的数字\n",
    "        before_decimals.append(digit)  # 将数字添加到列表中\n",
    "        val -= digit * base**(max_bit_pos - i - 1)  # 更新剩余的值\n",
    "\n",
    "    before_decimals = np.stack(before_decimals, axis=-1)  # 将列表转换为numpy数组\n",
    "\n",
    "    if prec > 0:  # 如果精度大于0\n",
    "        after_decimals = []  # 初始化小数点后的数字列表\n",
    "        for i in range(prec):  # 对于每个小数位\n",
    "            digit = (val / base**(-i - 1)).astype(int)  # 计算每位的数字\n",
    "            after_decimals.append(digit)  # 将数字添加到列表中\n",
    "            val -= digit * base**(-i - 1)  # 更新剩余的值\n",
    "\n",
    "        after_decimals = np.stack(after_decimals, axis=-1)  # 将列表转换为numpy数组\n",
    "        digits = np.concatenate([before_decimals, after_decimals], axis=-1)  # 将小数点前后的数字合并\n",
    "    else:\n",
    "        digits = before_decimals  # 如果精度为0，则只有小数点前的数字\n",
    "    return sign, digits  # 返回符号和数字\n",
    "\n",
    "def vec_repr2num(sign, digits, base, prec, half_bin_correction=True):  # 定义一个函数，将指定基数和精度的表示形式转换回数字\n",
    "    \"\"\"\n",
    "    将指定基数的字符串表示形式转换回数字。\n",
    "\n",
    "    参数:\n",
    "    - sign (np.array): 数字的符号。\n",
    "    - digits (np.array): 指定基数中的数字。\n",
    "    - base (int): 基数。\n",
    "    - prec (int): 小数点后的精度。\n",
    "    - half_bin_correction (bool): 如果为True，在数字上加上最小bin大小的0.5。\n",
    "\n",
    "    返回:\n",
    "    - np.array: 给定基数表示的数字对应的数组。\n",
    "    \"\"\"\n",
    "    base = float(base)  # 将基数转换为浮点数\n",
    "    bs, D = digits.shape  # 获取digits数组的形状\n",
    "    digits_flipped = np.flip(digits, axis=-1)  # 将digits数组在最后一个轴上翻转\n",
    "    powers = -np.arange(-prec, -prec + D)  # 计算每个位的幂次\n",
    "    val = np.sum(digits_flipped/base**powers, axis=-1)  # 计算转换后的数字\n",
    "\n",
    "    if half_bin_correction:  # 如果启用半bin校正\n",
    "        val += 0.5/base**prec  # 在数值上加上最小bin大小的一半\n",
    "\n",
    "    return sign * val  # 返回最终的数值数组\n",
    "\n",
    "@dataclass  # 使用dataclass装饰器定义一个数据类\n",
    "class SerializerSettings:  # 定义序列化数字的设置类\n",
    "    \"\"\"\n",
    "    数字序列化的设置。\n",
    "\n",
    "    属性:\n",
    "    - base (int): 数字表示的基数。\n",
    "    - prec (int): 小数点后的精度。\n",
    "    - signed (bool): 如果为True，允许负数。默认为False。\n",
    "    - fixed_length (bool): 如果为True，确保序列化字符串的固定长度。默认为False。\n",
    "    - max_val (float): 序列化的数字的最大绝对值。\n",
    "    - time_sep (str): 不同时间步的分隔符。\n",
    "    - bit_sep (str): 个别数字的分隔符。\n",
    "    - plus_sign (str): 正号的字符串表示。\n",
    "    - minus_sign (str): 负号的字符串表示。\n",
    "    - half_bin_correction (bool): 如果为True，在反序列化时应用半bin校正。默认为True。\n",
    "    - decimal_point (str): 小数点的字符串表示。\n",
    "    - missing_str (str): 缺失值的字符串表示。\n",
    "    \"\"\"\n",
    "    base: int = 10\n",
    "    prec: int = 3\n",
    "    signed: bool = True\n",
    "    fixed_length: bool = False\n",
    "    max_val: float = 1e7\n",
    "    time_sep: str = ' ,'\n",
    "    bit_sep: str = ' '\n",
    "    plus_sign: str = ''\n",
    "    minus_sign: str = ' -'\n",
    "    half_bin_correction: bool = True\n",
    "    decimal_point: str = ''\n",
    "    missing_str: str = ' Nan'\n",
    "\n",
    "def serialize_arr(arr, settings: SerializerSettings):  # 定义一个函数，根据提供的设置将数字数组序列化为字符串\n",
    "    \"\"\"\n",
    "    根据提供的设置将数字数组（时间序列）序列化为字符串。\n",
    "\n",
    "    参数:\n",
    "    - arr (np.array): 要序列化的数字数组。\n",
    "    - settings (SerializerSettings): 序列化的设置。\n",
    "\n",
    "    返回:\n",
    "    - str: 数组的字符串表示。\n",
    "    \"\"\"\n",
    "    # max_val仅用于固定nunm2repr中位数的数量，因此可以vmapped\n",
    "    assert np.all(np.abs(arr[~np.isnan(arr)]) <= settings.max_val), f\"abs(arr)必须<=max_val，\\\n",
    "         但abs(arr)={np.abs(arr)}, max_val={settings.max_val}\"\n",
    "    \n",
    "    if not settings.signed:  # 如果不允许负数\n",
    "        assert np.all(arr[~np.isnan(arr)] >= 0), f\"无符号数组必须>=0\"\n",
    "        plus_sign = minus_sign = ''\n",
    "    else:\n",
    "        plus_sign = settings.plus_sign\n",
    "        minus_sign = settings.minus_sign\n",
    "    \n",
    "    vnum2repr = partial(vec_num2repr,base=settings.base,prec=settings.prec,max_val=settings.max_val)  # 创建偏函数，准备进行数值到表示的转换\n",
    "    sign_arr, digits_arr = vnum2repr(np.where(np.isnan(arr),np.zeros_like(arr),arr))  # 对数组进行转换\n",
    "    ismissing = np.isnan(arr)  # 检查数组中的缺失值\n",
    "    \n",
    "    def tokenize(arr):  # 定义一个函数，用于将数组转换为字符串\n",
    "        return ''.join([settings.bit_sep+str(b) for b in arr])  # 将数组中的每个元素转换为字符串并连接\n",
    "    \n",
    "    bit_strs = []  # 初始化位字符串列表\n",
    "    for sign, digits,missing in zip(sign_arr, digits_arr,ismissing):  # 遍历每个数字的符号、位和是否缺失\n",
    "        if not settings.fixed_length:  # 如果不要求固定长度\n",
    "            # 移除前导零\n",
    "            nonzero_indices = np.where(digits != 0)[0]\n",
    "            if len(nonzero_indices) == 0:  # 如果全为零\n",
    "                digits = np.array([0])  # 只保留一个零\n",
    "            else:\n",
    "                digits = digits[nonzero_indices[0]:]  # 移除前导零\n",
    "            # 添加小数点\n",
    "            prec = settings.prec\n",
    "            if len(settings.decimal_point):\n",
    "                digits = np.concatenate([digits[:-prec], np.array([settings.decimal_point]), digits[-prec:]])\n",
    "        digits = tokenize(digits)  # 将数字数组转换为字符串\n",
    "        sign_sep = plus_sign if sign == 1 else minus_sign  # 根据符号选择符号分隔符\n",
    "        if missing:  # 如果该数值缺失\n",
    "            bit_strs.append(settings.missing_str)  # 添加缺失值字符串\n",
    "        else:\n",
    "            bit_strs.append(sign_sep + digits)  # 添加符号和数字字符串\n",
    "    bit_str = settings.time_sep.join(bit_strs)  # 使用时间分隔符连接所有数字字符串\n",
    "    bit_str += settings.time_sep  # 在末尾添加时间分隔符，避免最后一个时间步的位数不明确\n",
    "    return bit_str  # 返回序列化的字符串\n",
    "\n",
    "def deserialize_str(bit_str, settings: SerializerSettings, ignore_last=False, steps=None):  # 定义一个函数，根据提供的设置将字符串反序列化为数字数组\n",
    "    \"\"\"\n",
    "    根据提供的设置将字符串反序列化为数字数组（时间序列）。\n",
    "\n",
    "    参数:\n",
    "    - bit_str (str): 数字数组的字符串表示。\n",
    "    - settings (SerializerSettings): 反序列化的设置。\n",
    "    - ignore_last (bool): 如果为True，忽略字符串中的最后一个时间步（可能因为令牌限制等原因不完整）。默认为False。\n",
    "    - steps (int, optional): 要反序列化的步数或条目数。\n",
    "\n",
    "    返回:\n",
    "    - 如果反序列化第一个数字失败，则返回None；否则\n",
    "    - np.array: 对应于字符串的数字数组。\n",
    "    \"\"\"\n",
    "    # ignore_last用于忽略预测中的最后一个时间步，这个步骤经常由于令牌限制等原因而部分生成\n",
    "    orig_bitstring = bit_str  # 保存原始字符串\n",
    "    bit_strs = bit_str.split(settings.time_sep)  # 使用时间分隔符分割字符串\n",
    "    # 移除空字符串\n",
    "    bit_strs = [a for a in bit_strs if len(a) > 0]\n",
    "    if ignore_last:  # 如果忽略最后一个时间步\n",
    "        bit_strs = bit_strs[:-1]\n",
    "    if steps is not None:  # 如果指定了步数\n",
    "        bit_strs = bit_strs[:steps]\n",
    "    vrepr2num = partial(vec_repr2num, base=settings.base, prec=settings.prec, half_bin_correction=settings.half_bin_correction)  # 创建偏函数，准备进行表示到数值的转换\n",
    "    max_bit_pos = int(np.ceil(np.log(settings.max_val) / np.log(settings.base)).item())  # 计算最大值的位数\n",
    "    sign_arr = []  # 初始化符号数组\n",
    "    digits_arr = []  # 初始化数字数组\n",
    "    try:\n",
    "        for i, bit_str in enumerate(bit_strs):  # 遍历每个位字符串\n",
    "            if bit_str.startswith(settings.minus_sign):  # 如果以负号开始\n",
    "                sign = -1\n",
    "            elif bit_str.startswith(settings.plus_sign):  # 如果以正号开始\n",
    "                sign = 1\n",
    "            else:\n",
    "                assert settings.signed == False, f\"有符号的位字符串必须以{settings.minus_sign}或{settings.plus_sign}开始\"\n",
    "            bit_str = bit_str[len(settings.plus_sign):] if sign == 1 else bit_str[len(settings.minus_sign):]  # 去除符号\n",
    "            if settings.bit_sep == '':  # 如果位分隔符为空\n",
    "                bits = [b for b in bit_str.lstrip()]\n",
    "            else:\n",
    "                bits = [b[:1] for b in bit_str.lstrip().split(settings.bit_sep)]  # 根据位分隔符分割字符串\n",
    "            if settings.fixed_length:\n",
    "                # 检查位字符串是否具有固定长度\n",
    "                assert len(bits) == max_bit_pos + settings.prec, f\"固定长度的位字符串必须有{max_bit_pos + settings.prec}位，但有{len(bits)}位: '{bit_str}'\"\n",
    "            digits = []\n",
    "            for b in bits:\n",
    "                if b == settings.decimal_point:\n",
    "                    continue  # 跳过小数点\n",
    "                # 检查是否为数字\n",
    "                if b.isdigit():\n",
    "                    digits.append(int(b))\n",
    "                else:\n",
    "                    break  # 如果遇到非数字字符，则停止处理当前位字符串\n",
    "            sign_arr.append(sign)  # 添加当前数字的符号\n",
    "            digits_arr.append(digits)  # 添加当前数字的位\n",
    "    except Exception as e:\n",
    "        # 如果在反序列化过程中出现异常\n",
    "        print(f\"反序列化时出错 {settings.time_sep.join(bit_strs[i-2:i+5])}{settings.time_sep}\\n\\t{e}\")\n",
    "        print(f'原始字符串: {orig_bitstring}')\n",
    "        print(f\"位字符串: {bit_str}, 分隔符: {settings.bit_sep}\")\n",
    "        # 此时我们已经反序列化了部分位字符串，因此下面返回这些数字\n",
    "    if digits_arr:\n",
    "        # 添加前导零以使所有数字位数相同\n",
    "        max_len = max([len(d) for d in digits_arr])\n",
    "        for i in range(len(digits_arr)):\n",
    "            digits_arr[i] = [0] * (max_len - len(digits_arr[i])) + digits_arr[i]\n",
    "        # 使用sign_arr和digits_arr反序列化为数字数组\n",
    "        return vrepr2num(np.array(sign_arr), np.array(digits_arr))\n",
    "    else:\n",
    "        # 如果在第一步就出错，则返回None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd42ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库，用于深度学习模型\n",
    "import numpy as np  # 导入NumPy库，用于数值计算\n",
    "from jax import grad, vmap  # 从JAX库导入grad和vmap，用于自动微分和向量化映射\n",
    "from tqdm import tqdm  # 从tqdm库导入tqdm，用于显示进度条\n",
    "import argparse  # 导入argparse库，用于解析命令行参数\n",
    "from transformers import (  # 从transformers库导入LLaMA相关的类\n",
    "    LlamaForCausalLM, \n",
    "    LlamaTokenizer, \n",
    ")\n",
    "from data.serialize import serialize_arr, deserialize_str, SerializerSettings  # 从data.serialize导入序列化相关的函数和类\n",
    "\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"  # 定义默认的句尾标记\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"  # 定义默认的句首标记\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"  # 定义默认的未知词标记\n",
    "\n",
    "loaded = {}  # 初始化一个字典，用于缓存加载的模型\n",
    "\n",
    "def llama2_model_string(model_size, chat):  # 定义一个函数，用于生成LLaMA模型的字符串标识\n",
    "    chat = \"chat-\" if chat else \"\"  # 根据是否是聊天模式，添加相应的前缀\n",
    "    return f\"meta-llama/Llama-2-{model_size.lower()}-{chat}hf\"  # 返回模型的完整字符串标识\n",
    "\n",
    "def get_tokenizer(model):  # 定义一个函数，用于获取指定模型的分词器\n",
    "    name_parts = model.split(\"-\")  # 将模型名按照\"-\"分割\n",
    "    model_size = name_parts[0]  # 获取模型大小\n",
    "    chat = len(name_parts) > 1  # 判断是否是聊天模式\n",
    "    assert model_size in [\"7b\", \"13b\", \"70b\"]  # 断言模型大小是预定义的值之一\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(  # 从预训练的分词器中加载\n",
    "        llama2_model_string(model_size, chat),\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    special_tokens_dict = dict()  # 初始化一个字典，用于添加特殊标记\n",
    "    if tokenizer.eos_token is None:  # 如果没有句尾标记，添加默认的\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:  # 如果没有句首标记，添加默认的\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:  # 如果没有未知词标记，添加默认的\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)  # 将特殊标记添加到分词器中\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 将填充标记设置为句尾标记\n",
    "\n",
    "    return tokenizer  # 返回配置好的分词器\n",
    "\n",
    "def get_model_and_tokenizer(model_name, cache_model=False):  # 定义一个函数，用于获取指定模型名的模型和分词器\n",
    "    if model_name in loaded:  # 如果模型已经加载过，直接返回\n",
    "        return loaded[model_name]\n",
    "    name_parts = model_name.split(\"-\")  # 将模型名按照\"-\"分割\n",
    "    model_size = name_parts[0]  # 获取模型大小\n",
    "    chat = len(name_parts) > 1  # 判断是否是聊天模式\n",
    "\n",
    "    assert model_size in [\"7b\", \"13b\", \"70b\"]  # 断言模型大小是预定义的值之一\n",
    "\n",
    "    tokenizer = get_tokenizer(model_name)  # 获取分词器\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(  # 从预训练模型中加载LLaMA模型\n",
    "        llama2_model_string(model_size, chat),\n",
    "        device_map=\"auto\",   # 自动分配设备\n",
    "        torch_dtype=torch.float16,  # 使用半精度浮点数以节省内存\n",
    "    )\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    if cache_model:  # 如果启用了缓存模型\n",
    "        loaded[model_name] = (model, tokenizer)  # 缓存模型和分词器以便再次使用\n",
    "    return model, tokenizer  # 返回模型和分词器\n",
    "\n",
    "def tokenize_fn(str, model):  # 定义一个函数，用于对输入字符串进行分词\n",
    "    tokenizer = get_tokenizer(model)  # 获取对应模型的分词器\n",
    "    return tokenizer(str)  # 返回分词结果\n",
    "\n",
    "def llama_nll_fn(model, input_arr, target_arr, settings: SerializerSettings, transform, count_seps=True, temp=1, cache_model=True):\n",
    "    \"\"\"\n",
    "    计算目标数组（连续的）根据LM条件下的NLL/维度（以自然对数为底）。应用变换的相关对数行列式，\n",
    "    并通过假设在箱内均匀分布，将离散的NLL从LLM转换为连续。\n",
    "    输入：\n",
    "        input_arr: (n,) 上下文数组\n",
    "        target_arr: (n,) 真实数组\n",
    "        cache_model: 是否缓存模型和分词器以加快重复调用的速度\n",
    "    返回：NLL/D\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(model, cache_model=cache_model)  # 获取模型和分词器\n",
    "\n",
    "    input_str = serialize_arr(vmap(transform)(input_arr), settings)  # 将输入数组序列化为字符串\n",
    "    target_str = serialize_arr(vmap(transform)(target_arr), settings)  # 将目标数组序列化为字符串\n",
    "    full_series = input_str + target_str  # 将输入和目标字符串连接\n",
    "    \n",
    "    batch = tokenizer(  # 将字符串批量化处理\n",
    "        [full_series], \n",
    "        return_tensors=\"pt\",  # 返回PyTorch张量\n",
    "        add_special_tokens=True  # 添加特殊标记\n",
    "    )\n",
    "    batch = {k: v.cuda() for k, v in batch.items()}  # 将数据移动到GPU\n",
    "\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        out = model(**batch)  # 使用模型进行预测\n",
    "\n",
    "    good_tokens_str = list(\"0123456789\" + settings.time_sep)  # 定义有效的字符\n",
    "    good_tokens = [tokenizer.convert_tokens_to_ids(token) for token in good_tokens_str]  # 将有效字符转换为ID\n",
    "    bad_tokens = [i for i in range(len(tokenizer)) if i not in good_tokens]  # 定义无效字符的ID\n",
    "    out['logits'][:, :, bad_tokens] = -100  # 将无效字符的对数概率设为一个很小的值\n",
    "\n",
    "    input_ids = batch['input_ids'][0][1:]  # 获取输入的ID\n",
    "    logprobs = torch.nn.functional.log_softmax(out['logits'], dim=-1)[0][:-1]  # 计算对数概率\n",
    "    logprobs = logprobs[torch.arange(len(input_ids)), input_ids].cpu().numpy()  # 提取目标对数概率并转换为NumPy数组\n",
    "\n",
    "    tokens = tokenizer.batch_decode(  # 解码生成的ID\n",
    "        input_ids,\n",
    "        skip_special_tokens=False, \n",
    "        clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    input_len = len(tokenizer([input_str], return_tensors=\"pt\",)['input_ids'][0])  # 计算输入长度\n",
    "    input_len = input_len - 2 # 移除BOS标记\n",
    "\n",
    "    logprobs = logprobs[input_len:]  # 提取目标数组的对数概率\n",
    "    tokens = tokens[input_len:]  # 提取目标数组的标记\n",
    "    BPD = -logprobs.sum() / len(target_arr)  # 计算每维的负对数概率\n",
    "\n",
    "    # 计算调整后的BPD\n",
    "    transformed_nll = BPD - settings.prec * np.log(settings.base)  # 将离散的NLL转换为连续\n",
    "    avg_logdet_dydx = np.log(vmap(grad(transform))(target_arr)).mean()  # 计算变换的平均对数行列式\n",
    "    return transformed_nll - avg_logdet_dydx  # 返回变换后的NLL减去平均对数行列式\n",
    "\n",
    "def llama_completion_fn(\n",
    "    model,\n",
    "    input_str,\n",
    "    steps,\n",
    "    settings,\n",
    "    batch_size=5,\n",
    "    num_samples=20,\n",
    "    temp=0.9, \n",
    "    top_p=0.9,\n",
    "    cache_model=True\n",
    "):\n",
    "    \"\"\"\n",
    "    完成给定输入字符串的文本生成任务，根据设定的步数和其他参数生成文本。\n",
    "    输入：\n",
    "        model: 模型名\n",
    "        input_str: 输入字符串\n",
    "        steps: 生成步数\n",
    "        settings: 序列化设置\n",
    "        batch_size: 批处理大小\n",
    "        num_samples: 生成样本的数量\n",
    "        temp: 温度参数，控制生成多样性\n",
    "        top_p: 保留概率累积为top_p的最高概率词\n",
    "        cache_model: 是否缓存模型和分词器以加快重复调用的速度\n",
    "    返回：\n",
    "        生成的字符串列表\n",
    "    \"\"\"\n",
    "    avg_tokens_per_step = len(tokenize_fn(input_str, model)['input_ids']) / len(input_str.split(settings.time_sep))\n",
    "    max_tokens = int(avg_tokens_per_step * steps)  # 根据步数估计最大令牌数\n",
    "    \n",
    "    model, tokenizer = get_model_and_tokenizer(model, cache_model=cache_model)  # 获取模型和分词器\n",
    "\n",
    "    gen_strs = []  # 初始化生成字符串列表\n",
    "    for _ in tqdm(range(num_samples // batch_size)):  # 以批处理的方式生成文本\n",
    "        batch = tokenizer(\n",
    "            [input_str], \n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.repeat(batch_size, 1) for k, v in batch.items()}  # 复制批量输入\n",
    "        batch = {k: v.cuda() for k, v in batch.items()}  # 将数据移动到GPU\n",
    "        num_input_ids = batch['input_ids'].shape[1]  # 获取输入ID的数量\n",
    "\n",
    "        good_tokens_str = list(\"0123456789\" + settings.time_sep)  # 定义有效的字符\n",
    "        good_tokens = [tokenizer.convert_tokens_to_ids(token) for token in good_tokens_str]  # 将有效字符转换为ID\n",
    "        # good_tokens += [tokenizer.eos_token_id]  # 将EOS标记添加到有效标记列表中（此行被注释）\n",
    "        bad_tokens = [i for i in range(len(tokenizer)) if i not in good_tokens]  # 定义无效字符的ID\n",
    "\n",
    "        generate_ids = model.generate(\n",
    "            **batch,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temp, \n",
    "            top_p=top_p, \n",
    "            bad_words_ids=[[t] for t in bad_tokens],\n",
    "            renormalize_logits=True,\n",
    "        )  # 生成文本的ID\n",
    "        gen_strs += tokenizer.batch_decode(\n",
    "            generate_ids[:, num_input_ids:],\n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )  # 解码生成的ID，添加到字符串列表中\n",
    "    return gen_strs  # 返回生成的字符串列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a1f81-a10e-41b6-bca3-b2c9f4449114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from models.gpt import gpt_completion_fn, gpt_nll_fn\n",
    "from models.gpt import tokenize_fn as gpt_tokenize_fn\n",
    "from models.llama import llama_completion_fn, llama_nll_fn\n",
    "from models.llama import tokenize_fn as llama_tokenize_fn\n",
    "\n",
    "from models.mistral import mistral_completion_fn, mistral_nll_fn\n",
    "from models.mistral import tokenize_fn as mistral_tokenize_fn\n",
    "\n",
    "from models.mistral_api import mistral_api_completion_fn, mistral_api_nll_fn\n",
    "from models.mistral_api import tokenize_fn as mistral_api_tokenize_fn\n",
    "\n",
    "\n",
    "# Required: Text completion function for each model\n",
    "# -----------------------------------------------\n",
    "# Each model is mapped to a function that samples text completions.\n",
    "# The completion function should follow this signature:\n",
    "# \n",
    "# Args:\n",
    "#   - input_str (str): String representation of the input time series.\n",
    "#   - steps (int): Number of steps to predict.\n",
    "#   - settings (SerializerSettings): Serialization settings.\n",
    "#   - num_samples (int): Number of completions to sample.\n",
    "#   - temp (float): Temperature parameter for model's output randomness.\n",
    "# \n",
    "# Returns:\n",
    "#   - list: Sampled completion strings from the model.\n",
    "completion_fns = {\n",
    "    'gpt-3.5-turbo-instruct': partial(gpt_completion_fn, model='gpt-3.5-turbo-instruct'),\n",
    "    'gpt-4': partial(gpt_completion_fn, model='gpt-4'),\n",
    "    'gpt-4-1106-preview':partial(gpt_completion_fn, model='gpt-4-1106-preview'),\n",
    "    'gpt-3.5-turbo': partial(gpt_completion_fn, model='gpt-3.5-turbo'),\n",
    "    'mistral': partial(mistral_completion_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_completion_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_completion_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_completion_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_completion_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_completion_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_completion_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_completion_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_completion_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_completion_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: NLL/D functions for each model\n",
    "# -----------------------------------------------\n",
    "# Each model is mapped to a function that computes the continuous Negative Log-Likelihood \n",
    "# per Dimension (NLL/D). This is used for computing likelihoods only and not needed for sampling.\n",
    "# \n",
    "# The NLL function should follow this signature:\n",
    "# \n",
    "# Args:\n",
    "#   - input_arr (np.ndarray): Input time series (history) after data transformation.\n",
    "#   - target_arr (np.ndarray): Ground truth series (future) after data transformation.\n",
    "#   - settings (SerializerSettings): Serialization settings.\n",
    "#   - transform (callable): Data transformation function (e.g., scaling) for determining the Jacobian factor.\n",
    "#   - count_seps (bool): If True, count time step separators in NLL computation, required if allowing variable number of digits.\n",
    "#   - temp (float): Temperature parameter for sampling.\n",
    "# \n",
    "# Returns:\n",
    "#   - float: Computed NLL per dimension for p(target_arr | input_arr).\n",
    "nll_fns = {\n",
    "    'gpt-3.5-turbo-instruct': partial(gpt_nll_fn, model='gpt-3.5-turbo-instruct'),\n",
    "    'mistral': partial(mistral_nll_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_nll_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_nll_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_nll_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_completion_fn, model='7b'),\n",
    "    'llama-7b': partial(llama_nll_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_nll_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_nll_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_nll_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_nll_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_nll_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: Tokenization function for each model, only needed if you want automatic input truncation.\n",
    "# The tokenization function should follow this signature:\n",
    "#\n",
    "# Args:\n",
    "#   - str (str): A string to tokenize.\n",
    "# Returns:\n",
    "#   - token_ids (list): A list of token ids.\n",
    "tokenization_fns = {\n",
    "    'gpt-3.5-turbo-instruct': partial(gpt_tokenize_fn, model='gpt-3.5-turbo-instruct'),\n",
    "    'gpt-3.5-turbo': partial(gpt_tokenize_fn, model='gpt-3.5-turbo'),\n",
    "    'mistral': partial(mistral_tokenize_fn, model='mistral'),\n",
    "    'mistral-api-tiny': partial(mistral_api_tokenize_fn, model='mistral-tiny'),\n",
    "    'mistral-api-small': partial(mistral_api_tokenize_fn, model='mistral-small'),\n",
    "    'mistral-api-medium': partial(mistral_api_tokenize_fn, model='mistral-medium'),\n",
    "    'llama-7b': partial(llama_tokenize_fn, model='7b'),\n",
    "    'llama-13b': partial(llama_tokenize_fn, model='13b'),\n",
    "    'llama-70b': partial(llama_tokenize_fn, model='70b'),\n",
    "    'llama-7b-chat': partial(llama_tokenize_fn, model='7b-chat'),\n",
    "    'llama-13b-chat': partial(llama_tokenize_fn, model='13b-chat'),\n",
    "    'llama-70b-chat': partial(llama_tokenize_fn, model='70b-chat'),\n",
    "}\n",
    "\n",
    "# Optional: Context lengths for each model, only needed if you want automatic input truncation.\n",
    "context_lengths = {\n",
    "    'gpt-3.5-turbo-instruct': 4097,\n",
    "    'gpt-3.5-turbo': 4097,\n",
    "    'mistral-api-tiny': 4097,\n",
    "    'mistral-api-small': 4097,\n",
    "    'mistral-api-medium': 4097,\n",
    "    'mistral': 4096,\n",
    "    'llama-7b': 4096,\n",
    "    'llama-13b': 4096,\n",
    "    'llama-70b': 4096,\n",
    "    'llama-7b-chat': 4096,\n",
    "    'llama-13b-chat': 4096,\n",
    "    'llama-70b-chat': 4096,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ae2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import os\n",
    "import pickle\n",
    "from data.monash import get_datasets  # 用于获取时间序列数据集的函数\n",
    "from data.serialize import SerializerSettings  # 序列化设置的类\n",
    "from models.validation_likelihood_tuning import get_autotuned_predictions_data  # 自动调整预测数据的函数\n",
    "from models.utils import grid_iter  # 用于迭代超参数网格的函数\n",
    "from models.llmtime import get_llmtime_predictions_data  # 获取LLM时间序列预测数据的函数\n",
    "import numpy as np\n",
    "import openai\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']  # 从环境变量中读取OpenAI的API密钥\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")  # 设置OpenAI的API基础URL\n",
    "\n",
    "# 为每个模型指定超参数网格\n",
    "gpt3_hypers = dict(\n",
    "    temp=0.7,\n",
    "    alpha=0.9,\n",
    "    beta=0,\n",
    "    basic=False,\n",
    "    settings=SerializerSettings(base=10, prec=3, signed=True, half_bin_correction=True),\n",
    ")\n",
    "llama_hypers = dict(\n",
    "    temp=1.0,\n",
    "    alpha=0.99,\n",
    "    beta=0.3,\n",
    "    basic=False,\n",
    "    settings=SerializerSettings(base=10, prec=3, time_sep=',', bit_sep='', plus_sign='', minus_sign='-', signed=True), \n",
    ")\n",
    "model_hypers = {\n",
    "    'text-davinci-003': {'model': 'text-davinci-003', **gpt3_hypers},\n",
    "    'llama-7b': {'model': 'llama-7b', **llama_hypers},\n",
    "    'llama-70b': {'model': 'llama-70b', **llama_hypers},\n",
    "}\n",
    "\n",
    "# 为每个模型指定获取预测的函数\n",
    "model_predict_fns = {\n",
    "    'text-davinci-003': get_llmtime_predictions_data,\n",
    "    'llama-7b': get_llmtime_predictions_data,\n",
    "    'llama-70b': get_llmtime_predictions_data,\n",
    "}\n",
    "\n",
    "# 定义一个函数，用于检测模型是否为GPT系列\n",
    "def is_gpt(model):\n",
    "    return any([x in model for x in ['ada', 'babbage', 'curie', 'davinci', 'text-davinci-003', 'gpt-4']])\n",
    "\n",
    "# 指定保存结果的输出目录\n",
    "output_dir = 'outputs/monash'\n",
    "os.makedirs(output_dir, exist_ok=True)  # 如果目录不存在，则创建目录\n",
    "\n",
    "# 指定要运行的模型和数据集\n",
    "models_to_run = ['text-davinci-003']\n",
    "datasets_to_run =  [\n",
    "    \"weather\", \"covid_deaths\", \"solar_weekly\", \"tourism_monthly\", \"australian_electricity_demand\", \"pedestrian_counts\",\n",
    "    \"traffic_hourly\", \"hospital\", \"fred_md\", \"tourism_yearly\", \"tourism_quarterly\", \"us_births\",\n",
    "    \"nn5_weekly\", \"traffic_weekly\", \"saugeenday\", \"cif_2016\", \"bitcoin\", \"sunspot\", \"nn5_daily\"\n",
    "]\n",
    "\n",
    "max_history_len = 500  # 设置最大历史长度\n",
    "datasets = get_datasets()  # 获取所有数据集\n",
    "for dsname in datasets_to_run:  # 遍历要运行的数据集\n",
    "    print(f\"Starting {dsname}\")  # 打印开始信息\n",
    "    data = datasets[dsname]  # 获取数据集\n",
    "    train, test = data  # 分解数据集为训练和测试数据\n",
    "    train = [x[-max_history_len:] for x in train]  # 截取每个训练序列的最后max_history_len个点\n",
    "    if os.path.exists(f'{output_dir}/{dsname}.pkl'):  # 检查结果文件是否已存在\n",
    "        with open(f'{output_dir}/{dsname}.pkl','rb') as f:  # 如果存在，则加载之前的结果\n",
    "            out_dict = pickle.load(f)\n",
    "    else:\n",
    "        out_dict = {}  # 如果不存在，则初始化一个空字典用于存储结果\n",
    "\n",
    "    for model in models_to_run:  # 遍历要运行的模型\n",
    "        if model in out_dict:\n",
    "            print(f\"Skipping {dsname} {model}\")  # 如果当前模型的结果已存在，则跳过\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Starting {dsname} {model}\")  # 打印开始处理当前数据集和模型的信息\n",
    "            hypers = list(grid_iter(model_hypers[model]))  # 获取当前模型的超参数组合\n",
    "        parallel = True if is_gpt(model) else False  # 如果是GPT模型，则启用并行处理\n",
    "        num_samples = 5  # 设置预测样本数\n",
    "        \n",
    "        try:\n",
    "            # 调用get_autotuned_predictions_data函数进行预测\n",
    "            preds = get_autotuned_predictions_data(train, test, hypers, num_samples, model_predict_fns[model], verbose=False, parallel=parallel)\n",
    "            medians = preds['median']  # 获取预测的中位数\n",
    "            targets = np.array(test)  # 将测试数据转换为numpy数组\n",
    "            maes = np.mean(np.abs(medians - targets), axis=1)  # 计算每个时间序列的平均绝对误差(MAE)\n",
    "            preds['maes'] = maes  # 将MAE列表存储到预测结果字典中\n",
    "            preds['mae'] = np.mean(maes)  # 计算所有时间序列的MAE平均值，并存储到预测结果字典中\n",
    "            out_dict[model] = preds  # 将当前模型的预测结果存储到结果字典中\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {dsname} {model}\")  # 如果预测失败，则打印失败信息\n",
    "            print(e)  # 打印异常信息\n",
    "            continue  # 继续处理下一个模型\n",
    "        \n",
    "        # 保存当前数据集的预测结果到文件中\n",
    "        with open(f'{output_dir}/{dsname}.pkl', 'wb') as f:\n",
    "            pickle.dump(out_dict, f)\n",
    "    \n",
    "    print(f\"Finished {dsname}\")  # 打印完成当前数据集处理的信息\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
